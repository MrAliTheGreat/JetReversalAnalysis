2025-10-15-22:59:48.065396
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003992082977459233, Training R2: 0.9946004748344421
Validation Loss: 0.0006070060330997915, Validation R2: 0.9991686344146729

dataset: deterministic
bos_projector: linear
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 256
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 50
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-00:32:43.678788
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003974211549065002, Training R2: 0.9946238398551941
Validation Loss: 0.0005752544152857538, Validation R2: 0.9992530941963196

dataset: deterministic
bos_projector: linear
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 128
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 50
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-11:17:06.922253
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003958928469797078, Training R2: 0.9946452379226685
Validation Loss: 0.0005647616685080797, Validation R2: 0.9992684125900269

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 128
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 50
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-15:23:42.071613
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004239609274890994, Training R2: 0.9942638278007507
Validation Loss: 0.0007184310880628355, Validation R2: 0.9990106225013733

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 64
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 25
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-16:24:41.720617
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004221766575478371, Training R2: 0.9942877292633057
Validation Loss: 0.0005807439169965367, Validation R2: 0.9992120862007141

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 16
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 25
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-17:35:42.908619
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.0044233597527039694, Training R2: 0.9940106868743896
Validation Loss: 0.0005268601866617323, Validation R2: 0.9992832541465759

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 128
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 4
num_decoder_layers: 4
position_wise_nn_dim: 256
dropout: 0.25
batch_size: 128
epochs: 20
learning_rate: 0.0001
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-18:28:31.246456
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.0042580589224875225, Training R2: 0.9942380785942078
Validation Loss: 0.0006622097869849506, Validation R2: 0.9991266131401062

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 256
dropout: 0.25
batch_size: 128
epochs: 25
learning_rate: 0.0002
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-22:23:30.770919
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004768037123216332, Training R2: 0.9935474395751953
Validation Loss: 0.0014319063971770254, Validation R2: 0.9981160163879395

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state, mean of all input time-steps
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 10
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-00:31:39.412454 - Does not work!
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003965682891578858, Training R2: 0.9946367740631104
Validation Loss: 0.0006637677302623683, Validation R2: 0.9990925788879395

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 60
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-11:03:19.545982
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004239609274890994, Training R2: 0.9942638278007507
Validation Loss: 0.0007184310880628355, Validation R2: 0.9990106225013733

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 25
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-12:13:10.154224
./models/transformer-100-50-10.pt

Training Loss: 0.021881868947177835, Training R2: 0.9824151992797852
Validation Loss: 0.48440736273060675, Validation R2: 0.625281035900116

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 25
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-13:22:32.364438
./models/transformer-100-50-10.pt

Training Loss: 0.02019104216834907, Training R2: 0.9837352633476257
Validation Loss: 0.4651621889664931, Validation R2: 0.6413022875785828

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 25
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-13:49:59.943472
./models/transformer-deterministic-100-50-300.pt

Training Loss: -1.5705123699450116, Training R2: 0.9845497012138367
Validation Loss: -1.4896133919979664, Validation R2: 0.9969297051429749

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 15
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-16:33:51.486064 Overfitted!!!
./models/transformer-100-50-2.pt

Training Loss: -0.9428318089353007, Training R2: 0.9809757471084595
Validation Loss: 100.90923252590949, Validation R2: 0.6127330660820007

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 2
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 15
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-17:28:02.044492
./models/transformer-100-50-5.pt

Training Loss: -0.7840364223195796, Training R2: 0.9783315658569336
Validation Loss: 61.97019137083179, Validation R2: 0.5704159140586853

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 5
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 7
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-17:39:25.102290
./models/transformer-100-50-5.pt

Training Loss: -0.6493741360672733, Training R2: 0.9743571281433105
Validation Loss: 37.85112863619542, Validation R2: 0.5426886081695557

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 5
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 3
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-17:58:27.052473
./models/transformer-100-50-10.pt

Training Loss: -0.7027383898508266, Training R2: 0.9765169620513916
Validation Loss: 39.16112725631051, Validation R2: 0.6117148399353027

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 5
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-18:04:39.025102
./models/transformer-100-50-20.pt

Training Loss: -0.6460497023169377, Training R2: 0.9746063947677612
Validation Loss: 29.516263264876144, Validation R2: 0.6207616329193115

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 20
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 5
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-18:09:39.491383
./models/transformer-100-50-25.pt

Training Loss: -0.6214668698710938, Training R2: 0.9728001952171326
Validation Loss: 39.49353842508225, Validation R2: 0.5607725381851196

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 25
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 5
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-23-18:19:51.436156
./models/transformer-100-50-50.pt

Training Loss: -0.7411719675858816, Training R2: 0.9771260023117065
Validation Loss: 35.36924787001176, Validation R2: 0.6715399622917175

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 50
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-24-12:24:18.124534
./models/transformer-100-50-10.pt

Training Loss: -0.6323570067912465, Training R2: 0.974530816078186
Validation Loss: 32.407139755101596, Validation R2: 0.5945584177970886

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 3
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-24-12:44:32.343878
./models/transformer-200-50-10.pt

Training Loss: -0.6896029572822094, Training R2: 0.9767630696296692
Validation Loss: 42.23485252505443, Validation R2: 0.5793358683586121

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 200
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 5
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-24-12:59:39.683179
./models/transformer-50-50-10.pt

Training Loss: -0.7119453173514092, Training R2: 0.9765199422836304
Validation Loss: 33.26503161426004, Validation R2: 0.637596607208252

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: Gaussian negative log likelihood -> mean and var pred for each time-step
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 50
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 5
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-27-12:23:27.320846
./models/transformer-100-50-10-0.pt

Training Loss: 0.020862699126515595, Training R2: 0.9832220673561096
Validation Loss: 0.4520610667369216, Validation R2: 0.653794527053833

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 0
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-27-12:55:36.057869
./models/transformer-100-50-10-5.pt

Training Loss: 0.020855963658775244, Training R2: 0.9832144975662231
Validation Loss: 0.4480831965732114, Validation R2: 0.6569418907165527

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 5
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-27-13:26:59.135510
./models/transformer-100-50-10-10.pt

Training Loss: 0.020977260864429394, Training R2: 0.9831317663192749
Validation Loss: 0.44743848912381895, Validation R2: 0.6592037677764893

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 10
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-27-14:10:24.294978
./models/transformer-100-50-10-15.pt

Training Loss: 0.02078940832891308, Training R2: 0.983275830745697
Validation Loss: 0.4570969289915573, Validation R2: 0.6554281115531921

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 15
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-27-14:47:33.722464
./models/transformer-100-50-10-20.pt

Training Loss: 0.02081387650190412, Training R2: 0.9832426905632019
Validation Loss: 0.4162738103653498, Validation R2: 0.6782297492027283

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 20
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-28-15:15:41.046714
./models/transformer-100-50-10-25.pt

Training Loss: 0.02085708965764302, Training R2: 0.9832203388214111
Validation Loss: 0.4775786719460418, Validation R2: 0.6418531537055969

Per Feature R2:
    ['0.648242', '0.654381', '0.587752', '0.616562', '0.665956']

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 25
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-28-15:50:07.896695
./models/transformer-100-50-10-30.pt

Training Loss: 0.02088892276592376, Training R2: 0.9831941723823547
Validation Loss: 0.47635888319084607, Validation R2: 0.6337396502494812

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 30
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-11-03-12:56:34.630521
./models/transformer-200-100-10.pt

Training Loss: 0.023708339842657248, Training R2: 0.9825142621994019
Validation Loss: 1.0310416351789715, Validation R2: 0.3219274580478668

Per feature Pearson added as a train/val performance metric!
dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
Loss: MSE
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 200
label_window_len: 100
window_stride: 20
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 20
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list, eta_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================

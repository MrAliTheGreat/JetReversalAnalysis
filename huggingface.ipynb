{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8220bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/labnet5/gr5/abahari/Documents/Thesis/src/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import json, gc\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917c5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./params.json\", mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    model_path = data[\"model_path\"]\n",
    "    dataset_path = data[\"dataset_path\"]\n",
    "    num_single_sample_timesteps = data[\"num_single_sample_timesteps\"]\n",
    "    window_stride = data[\"window_stride\"]\n",
    "    input_window_length = data[\"input_window_length\"]\n",
    "    label_window_length = data[\"label_window_length\"]\n",
    "    input_features = data[\"input_features\"]\n",
    "    label_features = data[\"label_features\"]\n",
    "    positional_encoding_max_len = data[\"positional_encoding_max_len\"]\n",
    "    embedding_dim = data[\"embedding_dim\"]\n",
    "    num_attention_head = data[\"num_attention_head\"]\n",
    "    num_encoder_layers = data[\"num_encoder_layers\"]\n",
    "    num_decoder_layers = data[\"num_decoder_layers\"]\n",
    "    position_wise_nn_dim = data[\"position_wise_nn_dim\"]\n",
    "    dropout = data[\"dropout\"]\n",
    "    batch_size = data[\"batch_size\"]\n",
    "    epochs = data[\"epochs\"]\n",
    "    learning_rate = data[\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c01badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.regression import R2Score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_val = 0\n",
    "\n",
    "torch.manual_seed(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(dataset_path)\n",
    "\n",
    "df = df.drop([\"id\", \"eps\", \"n_0_squared\"])\n",
    "\n",
    "df = df.select(\n",
    "    pl.col(\"*\").str.json_decode()\n",
    ")\n",
    "\n",
    "df = df.with_columns(\n",
    "    eta_list = pl.col(\"eta_list\").list.eval(pl.element().flatten(), parallel = True)\n",
    ")\n",
    "\n",
    "df = df.sample(n = len(df), with_replacement = False, shuffle = True, seed = seed_val)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990f68c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape: (1050, 16)\n",
      "df_val shape: (300, 16)\n",
      "df_test shape: (150, 16)\n"
     ]
    }
   ],
   "source": [
    "df_train = df[:int(len(df) * 0.7)]\n",
    "df_val = df[int(len(df) * 0.7):int(len(df) * 0.9)]\n",
    "df_test = df[int(len(df) * 0.9):]\n",
    "\n",
    "print(f\"df_train shape: {df_train.shape}\")\n",
    "print(f\"df_val shape: {df_val.shape}\")\n",
    "print(f\"df_test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2546742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, input_df, label_df, num_single_sample_timesteps, stride, input_window_length, label_window_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_df = input_df  # Type: Numpy, Shape: Number of time-series, Number of time-steps, Number of input features\n",
    "        self.label_df = label_df  # Type: Numpy, Shape: Number of time-series, Number of time-steps, Number of label features\n",
    "        self.num_single_sample_timesteps = num_single_sample_timesteps\n",
    "        self.stride = stride\n",
    "        self.input_window_length = input_window_length\n",
    "        self.label_window_length = label_window_length\n",
    "\n",
    "        self.valid_length = self.input_window_length + self.label_window_length\n",
    "        \n",
    "        self.window_indices = []\n",
    "        for time_series_idx in range(self.input_df.shape[0]):\n",
    "            for input_window_start_idx in range(0, self.input_df.shape[1] - self.valid_length + 1, self.stride):\n",
    "                self.window_indices.append((time_series_idx, input_window_start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        time_series_idx, input_window_start_idx = self.window_indices[index]\n",
    "\n",
    "        label_window_start_idx = input_window_start_idx + self.input_window_length\n",
    "        input_window = self.input_df[time_series_idx, input_window_start_idx: label_window_start_idx, :]\n",
    "        label_window = self.label_df[time_series_idx, label_window_start_idx: label_window_start_idx + self.label_window_length, :]\n",
    "\n",
    "        input_window_mean = input_window.mean(axis = 0)\n",
    "        input_window_std = input_window.std(axis = 0)\n",
    "        input_window_std[input_window_std == 0] = 10 ** -8\n",
    "        input_window = (input_window - input_window_mean) / input_window_std\n",
    "\n",
    "        return torch.tensor(input_window, dtype = torch.float), torch.tensor(label_window, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33375a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### TRAIN #####\n",
    "\n",
    "num_train_samples = df_train.shape[0]\n",
    "\n",
    "input_df = df_train.select(\n",
    "    pl.col(input_features)\n",
    ")\n",
    "label_df = df_train.select(\n",
    "    pl.col(label_features)\n",
    ")\n",
    "input_df = input_df.explode(\"*\").to_numpy()\n",
    "input_df = input_df.reshape(num_train_samples, num_single_sample_timesteps, len(input_features))\n",
    "label_df = label_df.explode(\"*\").to_numpy()\n",
    "label_df = label_df.reshape(num_train_samples, num_single_sample_timesteps, len(label_features))\n",
    "\n",
    "df_train = WindowedDataset(\n",
    "    input_df = input_df,\n",
    "    label_df = label_df,\n",
    "    num_single_sample_timesteps = num_single_sample_timesteps,\n",
    "    stride = window_stride,\n",
    "    input_window_length = input_window_length,\n",
    "    label_window_length = label_window_length\n",
    ")\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    df_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 10,\n",
    "    prefetch_factor = 8,\n",
    "    persistent_workers = True,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "##### VALIDATION #####\n",
    "num_val_samples = df_val.shape[0]\n",
    "\n",
    "input_df = df_val.select(\n",
    "    pl.col(input_features)\n",
    ")\n",
    "label_df = df_val.select(\n",
    "    pl.col(label_features)\n",
    ")\n",
    "input_df = input_df.explode(\"*\").to_numpy()\n",
    "input_df = input_df.reshape(num_val_samples, num_single_sample_timesteps, len(input_features))\n",
    "label_df = label_df.explode(\"*\").to_numpy()\n",
    "label_df = label_df.reshape(num_val_samples, num_single_sample_timesteps, len(label_features))\n",
    "\n",
    "df_val = WindowedDataset(\n",
    "    input_df = input_df,\n",
    "    label_df = label_df,\n",
    "    num_single_sample_timesteps = num_single_sample_timesteps,\n",
    "    stride = window_stride,\n",
    "    input_window_length = input_window_length,\n",
    "    label_window_length = label_window_length\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    df_val,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 10,\n",
    "    prefetch_factor = 8,\n",
    "    persistent_workers = True,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "##### TEST #####\n",
    "num_test_samples = df_test.shape[0]\n",
    "\n",
    "input_df = df_test.select(\n",
    "    pl.col(input_features)\n",
    ")\n",
    "label_df = df_test.select(\n",
    "    pl.col(label_features)\n",
    ")\n",
    "input_df = input_df.explode(\"*\").to_numpy()\n",
    "input_df = input_df.reshape(num_test_samples, num_single_sample_timesteps, len(input_features))\n",
    "label_df = label_df.explode(\"*\").to_numpy()\n",
    "label_df = label_df.reshape(num_test_samples, num_single_sample_timesteps, len(label_features))\n",
    "\n",
    "df_test = WindowedDataset(\n",
    "    input_df = input_df,\n",
    "    label_df = label_df,\n",
    "    num_single_sample_timesteps = num_single_sample_timesteps,\n",
    "    stride = window_stride,\n",
    "    input_window_length = input_window_length,\n",
    "    label_window_length = label_window_length\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    df_test,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 10,\n",
    "    prefetch_factor = 8,\n",
    "    persistent_workers = True,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "##### CLEANING UP! #####\n",
    "\n",
    "del input_df\n",
    "del label_df\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdebf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "class TimeSeriesHuggingFaceTransformer(T5ForConditionalGeneration):\n",
    "    def __init__(self, input_dim, output_dim, d_model, num_head, num_encoder_layers, num_decoder_layers, position_wise_ffn_dim, dropout):\n",
    "        # batch_first = True in all huggingface models\n",
    "        config = T5Config(\n",
    "            vocab_size = 1, # No vocab --> = 1 is placeholder\n",
    "            d_model = d_model,\n",
    "            num_heads = num_head,\n",
    "            num_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            d_ff = position_wise_ffn_dim,\n",
    "            dropout = dropout,\n",
    "            decoder_start_token_id = 0,\n",
    "            tie_word_embeddings = False,\n",
    "            relative_attention_num_buckets = 32, \n",
    "            d_kv = d_model // num_head\n",
    "        )\n",
    "        \n",
    "        super().__init__(config)    # Creates model with random weights\n",
    "\n",
    "        self.encoder.embed_tokens = torch.nn.Linear(input_dim, d_model)     # Embedding layer for input\n",
    "        self.decoder.embed_tokens = torch.nn.Linear(output_dim, d_model)    # Embedding layer for output\n",
    "        \n",
    "        self.lm_head = torch.nn.Linear(d_model, output_dim, bias = False)   # Last linear before output\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, inputs_embeds, decoder_inputs_embeds, **kwargs):\n",
    "        outputs = super().forward(\n",
    "            inputs_embeds = inputs_embeds,\n",
    "            decoder_inputs_embeds = decoder_inputs_embeds,\n",
    "            **kwargs\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cfc6517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model: 133696\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 13962/13962 [03:25<00:00, 68.02it/s, train_loss=0.187408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.830562, Train R2: -106.674362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 3990/3990 [04:57<00:00, 13.41it/s, val_loss=27.133249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Val Loss: 20.650168, Val R2: -1.363798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.63it/s, train_loss=0.121980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 0.122387, Train R2: 0.637428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.32it/s, val_loss=18.938568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Val Loss: 19.201907, Val R2: -0.821573\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.78it/s, train_loss=0.095002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 0.105973, Train R2: 0.645899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 3990/3990 [04:58<00:00, 13.36it/s, val_loss=32.382839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Val Loss: 18.636787, Val R2: -0.342725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.65it/s, train_loss=0.087265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 0.100165, Train R2: 0.648567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.33it/s, val_loss=19.718063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Val Loss: 21.634004, Val R2: -5.790743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 13962/13962 [03:18<00:00, 70.45it/s, train_loss=0.110178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.097075, Train R2: 0.649805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.34it/s, val_loss=19.103989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Val Loss: 18.659587, Val R2: -0.055005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.69it/s, train_loss=0.085836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Loss: 0.094934, Train R2: 0.650467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 3990/3990 [05:00<00:00, 13.30it/s, val_loss=21.041477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Val Loss: 18.563221, Val R2: -0.311143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.70it/s, train_loss=0.090291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Loss: 0.093382, Train R2: 0.650861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 3990/3990 [05:00<00:00, 13.28it/s, val_loss=9.365047] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Val Loss: 18.317003, Val R2: -0.031908\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.52it/s, train_loss=0.095479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Train Loss: 0.092188, Train R2: 0.651369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 3990/3990 [04:58<00:00, 13.35it/s, val_loss=17.000731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Val Loss: 18.569779, Val R2: 0.006848\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.61it/s, train_loss=0.082175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Train Loss: 0.091313, Train R2: 0.651686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.32it/s, val_loss=14.443522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Val Loss: 18.304513, Val R2: 0.020815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.65it/s, train_loss=0.077977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.090598, Train R2: 0.652138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.31it/s, val_loss=43.441284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Val Loss: 18.591807, Val R2: 0.008229\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.79it/s, train_loss=0.090855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Train Loss: 0.089963, Train R2: 0.652152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 3990/3990 [04:58<00:00, 13.36it/s, val_loss=16.388966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Val Loss: 18.433795, Val R2: -0.044319\n",
      "\n",
      "Possible Overfitting!!! 1/3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.67it/s, train_loss=0.093056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Train Loss: 0.089468, Train R2: 0.652509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.32it/s, val_loss=16.331779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Val Loss: 18.302959, Val R2: -0.006938\n",
      "\n",
      "Possible Overfitting!!! 2/3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 13962/13962 [03:17<00:00, 70.83it/s, train_loss=0.082191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Train Loss: 0.088957, Train R2: 0.652704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 3990/3990 [04:59<00:00, 13.34it/s, val_loss=9.744701] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Val Loss: 18.348884, Val R2: -0.043417\n",
      "\n",
      "Possible Overfitting!!! 3/3\n",
      "\n",
      "Training Stopped!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesHuggingFaceTransformer(\n",
    "    input_dim = len(input_features),\n",
    "    output_dim = len(label_features),\n",
    "    d_model = embedding_dim,\n",
    "    num_head = num_attention_head,\n",
    "    num_encoder_layers = num_encoder_layers,\n",
    "    num_decoder_layers = num_encoder_layers,\n",
    "    position_wise_ffn_dim = position_wise_nn_dim,\n",
    "    dropout = dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"Number of trainable parameters in the model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\\n\")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate\n",
    ")\n",
    "\n",
    "train_r2 = R2Score(multioutput = \"uniform_average\").to(device)\n",
    "val_r2 = R2Score(multioutput = \"uniform_average\").to(device)\n",
    "\n",
    "overfit_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ################################################## TRAINING ##################################################\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    train_progress_bar = tqdm(\n",
    "        data_loader_train,\n",
    "        desc = f\"Epoch {epoch + 1}/{epochs}\"\n",
    "    )\n",
    "\n",
    "    for batch_x, batch_y in train_progress_bar:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            inputs_embeds = model.encoder.embed_tokens(batch_x),\n",
    "            decoder_inputs_embeds = model.decoder.embed_tokens(batch_y)\n",
    "        )\n",
    "\n",
    "        loss = criterion(outputs.logits, batch_y)   # logits is preds\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_r2.update(\n",
    "            outputs.logits.view(outputs.logits.shape[0], -1),\n",
    "            batch_y.view(batch_y.shape[0], -1)\n",
    "        )\n",
    "        train_progress_bar.set_postfix({\"train_loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(data_loader_train)\n",
    "    epoch_train_r2 = train_r2.compute()\n",
    "    train_r2.reset()\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Train R2: {epoch_train_r2:.6f}\")\n",
    "\n",
    "    ################################################# VALIDATION #################################################\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    val_progress_bar = tqdm(\n",
    "        data_loader_val,\n",
    "        desc = f\"Epoch {epoch + 1}/{epochs}\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_progress_bar:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            num_label_batch_samples, num_label_timesteps, num_label_features = batch_y.shape    # num_label_features == len(label_features)\n",
    "\n",
    "            encoder_outputs = model.encoder(\n",
    "                inputs_embeds = model.encoder.embed_tokens(batch_x)\n",
    "            )\n",
    "\n",
    "            bos = torch.zeros(\n",
    "                num_label_batch_samples, 1, num_label_features,\n",
    "                dtype = torch.float,\n",
    "                device = device\n",
    "            )\n",
    "            \n",
    "            preds = torch.zeros(\n",
    "                num_label_batch_samples, num_label_timesteps, num_label_features,\n",
    "                dtype = torch.float,\n",
    "                device = device\n",
    "            )\n",
    "            \n",
    "            ################### Analyze This ########################\n",
    "            # This is where KV caching is critical for speed.\n",
    "            past_key_values = None\n",
    "\n",
    "            for i in range(num_label_timesteps):\n",
    "                # 4. Pass the current decoder input to the decoder\n",
    "                # Use KV caching to only compute attention for the new token\n",
    "                decoder_outputs = model.decoder(\n",
    "                    inputs_embeds = model.decoder.embed_tokens(bos),\n",
    "                    encoder_hidden_states = encoder_outputs.last_hidden_state,\n",
    "                    past_key_values = past_key_values,\n",
    "                    use_cache = True,\n",
    "                    return_dict = True\n",
    "                )\n",
    "                \n",
    "                # 5. Extract the output for the *last* token\n",
    "                # This is the new prediction\n",
    "                decoder_last_hidden_state = decoder_outputs.last_hidden_state[:, -1:, :]\n",
    "\n",
    "                # 6. Apply the final linear layer (lm_head) to get the prediction\n",
    "                next_prediction = model.lm_head(decoder_last_hidden_state) # Shape: (batch_size, 1, num_label_features)\n",
    "\n",
    "                preds[:, i, :] = next_prediction.squeeze(1)\n",
    "\n",
    "                # 8. Update past_key_values for the next iteration\n",
    "                # This is the core of KV caching\n",
    "                past_key_values = decoder_outputs.past_key_values\n",
    "\n",
    "                # 9. The prediction for the current step becomes the input for the next step\n",
    "                bos = next_prediction\n",
    "            ################### Analyze This ########################\n",
    "\n",
    "\n",
    "            loss = criterion(preds, batch_y)\n",
    "            epoch_val_loss += loss.item()\n",
    "            val_r2.update(\n",
    "                preds.view(preds.shape[0], -1),\n",
    "                batch_y.view(batch_y.shape[0], -1)\n",
    "            )\n",
    "            val_progress_bar.set_postfix({\"val_loss\": f\"{loss.item():.6f}\"})\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(data_loader_val)\n",
    "    epoch_val_r2 = val_r2.compute()\n",
    "    val_r2.reset()\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Val Loss: {avg_val_loss:.6f}, Val R2: {epoch_val_r2:.6f}\\n\")\n",
    "\n",
    "\n",
    "    if(epoch >= 10 and avg_val_loss - avg_train_loss > 0.01):\n",
    "        overfit_count += 1\n",
    "        print(f\"Possible Overfitting!!! {overfit_count}/3\\n\")\n",
    "        if(overfit_count == 3):\n",
    "            print(\"Training Stopped!!!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1998dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda52e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803093a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

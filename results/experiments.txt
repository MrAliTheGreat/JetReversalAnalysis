2025-10-15-22:59:48.065396
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003992082977459233, Training R2: 0.9946004748344421
Validation Loss: 0.0006070060330997915, Validation R2: 0.9991686344146729

dataset: deterministic
bos_projector: linear
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 256
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 50
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-00:32:43.678788
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003974211549065002, Training R2: 0.9946238398551941
Validation Loss: 0.0005752544152857538, Validation R2: 0.9992530941963196

dataset: deterministic
bos_projector: linear
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 128
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 50
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-11:17:06.922253
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003958928469797078, Training R2: 0.9946452379226685
Validation Loss: 0.0005647616685080797, Validation R2: 0.9992684125900269

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 128
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 50
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-15:23:42.071613
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004239609274890994, Training R2: 0.9942638278007507
Validation Loss: 0.0007184310880628355, Validation R2: 0.9990106225013733

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 64
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 25
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-16:24:41.720617
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004221766575478371, Training R2: 0.9942877292633057
Validation Loss: 0.0005807439169965367, Validation R2: 0.9992120862007141

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
relative_attention_num_buckets: 16
input_window_len: 100
label_window_len: 50
window_stride: 300
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
batch_size: 128
epochs: 25
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-17:35:42.908619
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.0044233597527039694, Training R2: 0.9940106868743896
Validation Loss: 0.0005268601866617323, Validation R2: 0.9992832541465759

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 128
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 4
num_decoder_layers: 4
position_wise_nn_dim: 256
dropout: 0.25
batch_size: 128
epochs: 20
learning_rate: 0.0001
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-18:28:31.246456
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.0042580589224875225, Training R2: 0.9942380785942078
Validation Loss: 0.0006622097869849506, Validation R2: 0.9991266131401062

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 256
dropout: 0.25
batch_size: 128
epochs: 25
learning_rate: 0.0002
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-16-22:23:30.770919
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004768037123216332, Training R2: 0.9935474395751953
Validation Loss: 0.0014319063971770254, Validation R2: 0.9981160163879395

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state, mean of all input time-steps
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 10
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-00:31:39.412454 - Does not work!
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.003965682891578858, Training R2: 0.9946367740631104
Validation Loss: 0.0006637677302623683, Validation R2: 0.9990925788879395

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 60
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-11:03:19.545982
./models/transformer-deterministic-100-50-300.pt

Training Loss: 0.004239609274890994, Training R2: 0.9942638278007507
Validation Loss: 0.0007184310880628355, Validation R2: 0.9990106225013733

dataset: deterministic
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 300
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 25
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-12:13:10.154224
./models/transformer-100-50-10.pt

Training Loss: 0.021881868947177835, Training R2: 0.9824151992797852
Validation Loss: 0.48440736273060675, Validation R2: 0.625281035900116

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 25
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================
2025-10-17-13:22:32.364438
./models/transformer-100-50-10.pt

Training Loss: 0.02019104216834907, Training R2: 0.9837352633476257
Validation Loss: 0.4651621889664931, Validation R2: 0.6413022875785828

dataset: random
bos_projector: non-linear (1 LeakyReLU)
bos_input: encoder hidden state of last input time-step
positional encoding: sin, cos
seed_val: 7
num_single_sample_timesteps: 1000
input_window_len: 100
label_window_len: 50
window_stride: 10
relative_attention_num_buckets: 64
embedding_dim: 128
num_attention_head: 8
num_encoder_layers: 5
num_decoder_layers: 5
position_wise_nn_dim: 64
dropout: 0.5
batch_size: 128
epochs: 25
learning_rate: 0.0005
psi_e, b_e, psi_plus, b_plus, u_list -> psi_e, b_e, psi_plus, b_plus, u_list

==========================================

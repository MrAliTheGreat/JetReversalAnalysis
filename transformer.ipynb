{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With VS Code Jupyter you don't need to add the kernel to Jupyter Notebook like before to have it as an environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/labnet5/gr5/abahari/Documents/Thesis/src/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import json, gc\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./params.json\", mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    model_path = data[\"model_path\"]\n",
    "    num_single_sample_timesteps = data[\"num_single_sample_timesteps\"]\n",
    "    window_stride = data[\"window_stride\"]\n",
    "    input_window_length = data[\"input_window_length\"]\n",
    "    label_window_length = data[\"label_window_length\"]\n",
    "    input_features = data[\"input_features\"]\n",
    "    label_features = data[\"label_features\"]\n",
    "    positional_encoding_max_len = data[\"positional_encoding_max_len\"]\n",
    "    embedding_dim = data[\"embedding_dim\"]\n",
    "    num_attention_head = data[\"num_attention_head\"]\n",
    "    num_encoder_layers = data[\"num_encoder_layers\"]\n",
    "    num_decoder_layers = data[\"num_decoder_layers\"]\n",
    "    position_wise_nn_dim = data[\"position_wise_nn_dim\"]\n",
    "    dropout = data[\"dropout\"]\n",
    "    batch_size = data[\"batch_size\"]\n",
    "    epochs = data[\"epochs\"]\n",
    "    learning_rate = data[\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_500, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>eps</th><th>n_0_squared</th><th>psi_e</th><th>b_e</th><th>psi_plus</th><th>b_plus</th><th>u_list</th><th>r_list</th><th>k_e_psi_e_list</th><th>k_e_b_e_list</th><th>k_e_psi_plus_list</th><th>k_e_b_plus_list</th><th>heat_flux_psi_e_b_e_list</th><th>heat_flux_psi_e_b_plus_list</th><th>b_e_psi_plus_list</th><th>b_e_b_plus_list</th><th>psi_plus_b_plus_list</th><th>eta_list</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1500</td><td>0.123943</td><td>318.864022</td><td>&quot;[-0.011744609035812982, -0.011…</td><td>&quot;[-5.517162102724528, -3.192070…</td><td>&quot;[-0.0016475367466768115, -0.00…</td><td>&quot;[10.73076649947905, 10.7758112…</td><td>&quot;[0.4615413218450946, 0.4686702…</td><td>&quot;[0.6551578220909112, 0.8945764…</td><td>&quot;[0.00013793584140409996, 0.000…</td><td>&quot;[30.43907766773974, 10.1893158…</td><td>&quot;[2.7143773316504123e-06, 5.549…</td><td>&quot;[115.14934966634185, 116.11810…</td><td>&quot;[0.06479691188370344, 0.035800…</td><td>&quot;[-0.1260286571909809, -0.12085…</td><td>&quot;[0.009089727301611367, 0.00751…</td><td>&quot;[-59.203378264111755, -34.3971…</td><td>&quot;[-0.01767933212790023, -0.0253…</td><td>&quot;[[1.0369206541904592], [-1.681…</td></tr><tr><td>1501</td><td>0.123943</td><td>318.864022</td><td>&quot;[-0.008147157474100065, -0.008…</td><td>&quot;[-0.832904729882089, 0.1407146…</td><td>&quot;[0.0006807256129005129, 0.0007…</td><td>&quot;[-0.7190970906032216, -0.60951…</td><td>&quot;[0.23703131704960073, 0.234794…</td><td>&quot;[-0.18778048603775238, -0.2053…</td><td>&quot;[6.637617490778455e-05, 6.8758…</td><td>&quot;[0.6937302890599556, 0.0198006…</td><td>&quot;[4.6338736005877887e-07, 5.349…</td><td>&quot;[0.5171006257140178, 0.3715125…</td><td>&quot;[0.006785805995272157, -0.0011…</td><td>&quot;[0.005858597236311648, 0.00505…</td><td>&quot;[-0.0005669795827367211, 0.000…</td><td>&quot;[0.5989393680078723, -0.085768…</td><td>&quot;[-0.0004895078077358536, -0.00…</td><td>&quot;[[1.418213787060706], [0.88278…</td></tr><tr><td>1502</td><td>0.123943</td><td>318.864022</td><td>&quot;[-0.003781434990007615, -0.001…</td><td>&quot;[7.429632504232679, 7.61617440…</td><td>&quot;[0.0017201097552821953, 0.0016…</td><td>&quot;[0.26192796713213556, -0.01619…</td><td>&quot;[0.3398928494975913, 0.3384786…</td><td>&quot;[-0.22023434840890527, -0.0752…</td><td>&quot;[1.4299250583653892e-05, 1.712…</td><td>&quot;[55.199439147950756, 58.006112…</td><td>&quot;[2.958777570216974e-06, 2.8808…</td><td>&quot;[0.06860625996597308, 0.000262…</td><td>&quot;[-0.028094672314403355, -0.009…</td><td>&quot;[-0.000990463579775022, 2.1190…</td><td>&quot;[0.012779783348692317, 0.01292…</td><td>&quot;[1.9460285383725033, -0.123315…</td><td>&quot;[0.00045054485144522063, -2.74…</td><td>&quot;[[-0.6679438799496836], [-0.80…</td></tr><tr><td>1503</td><td>0.123943</td><td>318.864022</td><td>&quot;[-0.008102021680746106, -0.007…</td><td>&quot;[-13.405475487140006, -12.1429…</td><td>&quot;[-0.007065355563569785, -0.007…</td><td>&quot;[3.432058579711073, 2.92797364…</td><td>&quot;[0.15021463519080008, 0.170829…</td><td>&quot;[1.9382048681079675, 1.8501503…</td><td>&quot;[6.564275531527996e-05, 5.7415…</td><td>&quot;[179.70677303631157, 147.45183…</td><td>&quot;[4.991924923966652e-05, 5.2004…</td><td>&quot;[11.779026094568387, 8.5730296…</td><td>&quot;[0.10861145303751879, 0.092010…</td><td>&quot;[-0.0278066130224098, -0.02218…</td><td>&quot;[0.09471445081536302, 0.087568…</td><td>&quot;[-46.00837716074533, -35.55430…</td><td>&quot;[-0.024248714180659044, -0.021…</td><td>&quot;[[0.20668630436664126], [1.034…</td></tr><tr><td>1504</td><td>0.123943</td><td>318.864022</td><td>&quot;[-0.0007052290130828054, -0.00…</td><td>&quot;[7.188510348955567, 7.25531527…</td><td>&quot;[0.01155266108186768, 0.011468…</td><td>&quot;[-0.9665805895951766, 0.236189…</td><td>&quot;[0.1388431307750241, 0.1327890…</td><td>&quot;[-0.2758572865676108, -1.34969…</td><td>&quot;[4.973479608937477e-07, 1.2082…</td><td>&quot;[51.67468103704128, 52.6395997…</td><td>&quot;[0.0001334639780725001, 0.0001…</td><td>&quot;[0.9342780361821591, 0.0557856…</td><td>&quot;[-0.0050695460589294675, -0.02…</td><td>&quot;[0.0006816606752652025, -0.000…</td><td>&quot;[0.08304642374498203, 0.083205…</td><td>&quot;[-6.9482745714045, 1.713632080…</td><td>&quot;[-0.011166577959904913, 0.0027…</td><td>&quot;[[-0.2944195173353409], [0.846…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2995</td><td>0.123943</td><td>318.864022</td><td>&quot;[0.0027304602552334, -0.002678…</td><td>&quot;[-15.816264644897496, -15.5200…</td><td>&quot;[0.0012142394002116878, 0.0009…</td><td>&quot;[2.764721975700982, 4.05941343…</td><td>&quot;[0.4027182522813478, 0.4025100…</td><td>&quot;[0.11225674279144691, -0.08758…</td><td>&quot;[7.455413205409244e-06, 7.1762…</td><td>&quot;[250.1542273174345, 240.873016…</td><td>&quot;[1.4743773210264393e-06, 9.324…</td><td>&quot;[7.643687602923941, 16.4788374…</td><td>&quot;[-0.04318568199914582, 0.04157…</td><td>&quot;[0.007548963471421893, -0.0108…</td><td>&quot;[-0.01920473169600966, -0.0149…</td><td>&quot;[-43.727574437250595, -63.0024…</td><td>&quot;[0.0033570343535272327, 0.0039…</td><td>&quot;[[-1.3447105080916295], [-0.78…</td></tr><tr><td>2996</td><td>0.123943</td><td>318.864022</td><td>&quot;[0.0038198007840869012, 0.0010…</td><td>&quot;[-6.111776214911955, -6.408017…</td><td>&quot;[0.002738456236028401, 0.00267…</td><td>&quot;[0.1260447413606365, 0.8997889…</td><td>&quot;[0.3815429695058497, 0.3839093…</td><td>&quot;[0.35417571126001496, 0.096388…</td><td>&quot;[1.4590878030110906e-05, 1.135…</td><td>&quot;[37.3538085011635, 41.06268254…</td><td>&quot;[7.4991425566428376e-06, 7.135…</td><td>&quot;[0.01588727682466975, 0.809620…</td><td>&quot;[-0.02334576757788436, -0.0068…</td><td>&quot;[0.00048146580187939, 0.000958…</td><td>&quot;[-0.0167368316889357, -0.01711…</td><td>&quot;[-0.7703572522626673, -5.76586…</td><td>&quot;[0.00034516800799762196, 0.002…</td><td>&quot;[[-0.08993232103131506], [-1.1…</td></tr><tr><td>2997</td><td>0.123943</td><td>318.864022</td><td>&quot;[-0.007867764494778443, -0.010…</td><td>&quot;[4.615399916306391, 5.52828332…</td><td>&quot;[-0.0027620958841561424, -0.00…</td><td>&quot;[-3.0540710897004004, -3.64880…</td><td>&quot;[0.32570331855224116, 0.334151…</td><td>&quot;[0.7358043633897454, 0.8418703…</td><td>&quot;[6.19017181452963e-05, 0.00010…</td><td>&quot;[21.30191638744104, 30.5619165…</td><td>&quot;[7.629173673272302e-06, 6.1743…</td><td>&quot;[9.32735022094379, 13.31377581…</td><td>&quot;[-0.03631287959071882, -0.0553…</td><td>&quot;[0.02402871208407412, 0.036511…</td><td>&quot;[-0.012748177112564487, -0.013…</td><td>&quot;[-14.095759451796996, -20.1716…</td><td>&quot;[0.00843563718678174, 0.009066…</td><td>&quot;[[-1.748145068158444], [0.0966…</td></tr><tr><td>2998</td><td>0.123943</td><td>318.864022</td><td>&quot;[0.004137980019488926, 0.00952…</td><td>&quot;[20.850303575856653, 19.417738…</td><td>&quot;[-0.006889452280651537, -0.006…</td><td>&quot;[-2.251806224644361, -5.487241…</td><td>&quot;[0.6490342140491646, 0.6324470…</td><td>&quot;[-0.9652622949842546, -2.12360…</td><td>&quot;[1.7122878641689577e-05, 9.077…</td><td>&quot;[434.73515920538074, 377.04856…</td><td>&quot;[4.746455272737466e-05, 4.3332…</td><td>&quot;[5.07063127334709, 30.10982431…</td><td>&quot;[0.08627813959717334, 0.185009…</td><td>&quot;[-0.00931792916533916, -0.0522…</td><td>&quot;[-0.1436471715229625, -0.12782…</td><td>&quot;[-46.950843377838595, -106.549…</td><td>&quot;[0.01551371152996142, 0.036121…</td><td>&quot;[[-1.702330860987556], [-0.836…</td></tr><tr><td>2999</td><td>0.123943</td><td>318.864022</td><td>&quot;[0.0016221210706882298, 0.0003…</td><td>&quot;[10.740295731650054, 10.496021…</td><td>&quot;[0.004209897532959411, 0.00425…</td><td>&quot;[-1.3343152924869617, -1.13226…</td><td>&quot;[0.15890387261007743, 0.159762…</td><td>&quot;[0.23122087882402925, 0.050659…</td><td>&quot;[2.631276767970729e-06, 1.2369…</td><td>&quot;[115.35395240330037, 110.16647…</td><td>&quot;[1.7723237238017733e-05, 1.809…</td><td>&quot;[1.780397299764566, 1.28201614…</td><td>&quot;[0.01742206001173241, 0.003691…</td><td>&quot;[-0.0021644209508846286, -0.00…</td><td>&quot;[0.04521554450392805, 0.044652…</td><td>&quot;[-14.330940840573108, -11.8842…</td><td>&quot;[-0.005617330658030875, -0.004…</td><td>&quot;[[0.470845347289016], [-0.7656…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_500, 19)\n",
       "┌──────┬──────────┬────────────┬────────────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
       "│ id   ┆ eps      ┆ n_0_square ┆ psi_e      ┆ … ┆ b_e_psi_pl ┆ b_e_b_plus ┆ psi_plus_b ┆ eta_list  │\n",
       "│ ---  ┆ ---      ┆ d          ┆ ---        ┆   ┆ us_list    ┆ _list      ┆ _plus_list ┆ ---       │\n",
       "│ i64  ┆ f64      ┆ ---        ┆ str        ┆   ┆ ---        ┆ ---        ┆ ---        ┆ str       │\n",
       "│      ┆          ┆ f64        ┆            ┆   ┆ str        ┆ str        ┆ str        ┆           │\n",
       "╞══════╪══════════╪════════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
       "│ 1500 ┆ 0.123943 ┆ 318.864022 ┆ [-0.011744 ┆ … ┆ [0.0090897 ┆ [-59.20337 ┆ [-0.017679 ┆ [[1.03692 │\n",
       "│      ┆          ┆            ┆ 6090358129 ┆   ┆ 2730161136 ┆ 8264111755 ┆ 3321279002 ┆ 065419045 │\n",
       "│      ┆          ┆            ┆ 82,        ┆   ┆ 7,         ┆ ,          ┆ 3,         ┆ 92],      │\n",
       "│      ┆          ┆            ┆ -0.011…    ┆   ┆ 0.00751…   ┆ -34.3971…  ┆ -0.0253…   ┆ [-1.681…  │\n",
       "│ 1501 ┆ 0.123943 ┆ 318.864022 ┆ [-0.008147 ┆ … ┆ [-0.000566 ┆ [0.5989393 ┆ [-0.000489 ┆ [[1.41821 │\n",
       "│      ┆          ┆            ┆ 1574741000 ┆   ┆ 9795827367 ┆ 680078723, ┆ 5078077358 ┆ 378706070 │\n",
       "│      ┆          ┆            ┆ 65,        ┆   ┆ 211,       ┆ -0.085768… ┆ 536,       ┆ 6],       │\n",
       "│      ┆          ┆            ┆ -0.008…    ┆   ┆ 0.000…     ┆            ┆ -0.00…     ┆ [0.88278… │\n",
       "│ 1502 ┆ 0.123943 ┆ 318.864022 ┆ [-0.003781 ┆ … ┆ [0.0127797 ┆ [1.9460285 ┆ [0.0004505 ┆ [[-0.6679 │\n",
       "│      ┆          ┆            ┆ 4349900076 ┆   ┆ 8334869231 ┆ 383725033, ┆ 4485144522 ┆ 438799496 │\n",
       "│      ┆          ┆            ┆ 15,        ┆   ┆ 7,         ┆ -0.123315… ┆ 063,       ┆ 836],     │\n",
       "│      ┆          ┆            ┆ -0.001…    ┆   ┆ 0.01292…   ┆            ┆ -2.74…     ┆ [-0.80…   │\n",
       "│ 1503 ┆ 0.123943 ┆ 318.864022 ┆ [-0.008102 ┆ … ┆ [0.0947144 ┆ [-46.00837 ┆ [-0.024248 ┆ [[0.20668 │\n",
       "│      ┆          ┆            ┆ 0216807461 ┆   ┆ 5081536302 ┆ 716074533, ┆ 7141806590 ┆ 630436664 │\n",
       "│      ┆          ┆            ┆ 06,        ┆   ┆ ,          ┆ -35.55430… ┆ 44,        ┆ 126],     │\n",
       "│      ┆          ┆            ┆ -0.007…    ┆   ┆ 0.087568…  ┆            ┆ -0.021…    ┆ [1.034…   │\n",
       "│ 1504 ┆ 0.123943 ┆ 318.864022 ┆ [-0.000705 ┆ … ┆ [0.0830464 ┆ [-6.948274 ┆ [-0.011166 ┆ [[-0.2944 │\n",
       "│      ┆          ┆            ┆ 2290130828 ┆   ┆ 2374498203 ┆ 5714045,   ┆ 5779599049 ┆ 195173353 │\n",
       "│      ┆          ┆            ┆ 054,       ┆   ┆ ,          ┆ 1.71363208 ┆ 13,        ┆ 409],     │\n",
       "│      ┆          ┆            ┆ -0.00…     ┆   ┆ 0.083205…  ┆ 0…         ┆ 0.0027…    ┆ [0.846…   │\n",
       "│ …    ┆ …        ┆ …          ┆ …          ┆ … ┆ …          ┆ …          ┆ …          ┆ …         │\n",
       "│ 2995 ┆ 0.123943 ┆ 318.864022 ┆ [0.0027304 ┆ … ┆ [-0.019204 ┆ [-43.72757 ┆ [0.0033570 ┆ [[-1.3447 │\n",
       "│      ┆          ┆            ┆ 602552334, ┆   ┆ 7316960096 ┆ 4437250595 ┆ 3435352723 ┆ 105080916 │\n",
       "│      ┆          ┆            ┆ -0.002678… ┆   ┆ 6,         ┆ ,          ┆ 27,        ┆ 295],     │\n",
       "│      ┆          ┆            ┆            ┆   ┆ -0.0149…   ┆ -63.0024…  ┆ 0.0039…    ┆ [-0.78…   │\n",
       "│ 2996 ┆ 0.123943 ┆ 318.864022 ┆ [0.0038198 ┆ … ┆ [-0.016736 ┆ [-0.770357 ┆ [0.0003451 ┆ [[-0.0899 │\n",
       "│      ┆          ┆            ┆ 0078408690 ┆   ┆ 8316889357 ┆ 2522626673 ┆ 6800799762 ┆ 323210313 │\n",
       "│      ┆          ┆            ┆ 12,        ┆   ┆ ,          ┆ ,          ┆ 196,       ┆ 1506],    │\n",
       "│      ┆          ┆            ┆ 0.0010…    ┆   ┆ -0.01711…  ┆ -5.76586…  ┆ 0.002…     ┆ [-1.1…    │\n",
       "│ 2997 ┆ 0.123943 ┆ 318.864022 ┆ [-0.007867 ┆ … ┆ [-0.012748 ┆ [-14.09575 ┆ [0.0084356 ┆ [[-1.7481 │\n",
       "│      ┆          ┆            ┆ 7644947784 ┆   ┆ 1771125644 ┆ 9451796996 ┆ 3718678174 ┆ 450681584 │\n",
       "│      ┆          ┆            ┆ 43,        ┆   ┆ 87,        ┆ ,          ┆ ,          ┆ 44],      │\n",
       "│      ┆          ┆            ┆ -0.010…    ┆   ┆ -0.013…    ┆ -20.1716…  ┆ 0.009066…  ┆ [0.0966…  │\n",
       "│ 2998 ┆ 0.123943 ┆ 318.864022 ┆ [0.0041379 ┆ … ┆ [-0.143647 ┆ [-46.95084 ┆ [0.0155137 ┆ [[-1.7023 │\n",
       "│      ┆          ┆            ┆ 8001948892 ┆   ┆ 1715229625 ┆ 3377838595 ┆ 1152996142 ┆ 308609875 │\n",
       "│      ┆          ┆            ┆ 6,         ┆   ┆ ,          ┆ ,          ┆ ,          ┆ 56],      │\n",
       "│      ┆          ┆            ┆ 0.00952…   ┆   ┆ -0.12782…  ┆ -106.549…  ┆ 0.036121…  ┆ [-0.836…  │\n",
       "│ 2999 ┆ 0.123943 ┆ 318.864022 ┆ [0.0016221 ┆ … ┆ [0.0452155 ┆ [-14.33094 ┆ [-0.005617 ┆ [[0.47084 │\n",
       "│      ┆          ┆            ┆ 2107068822 ┆   ┆ 4450392805 ┆ 0840573108 ┆ 3306580308 ┆ 534728901 │\n",
       "│      ┆          ┆            ┆ 98,        ┆   ┆ ,          ┆ ,          ┆ 75,        ┆ 6],       │\n",
       "│      ┆          ┆            ┆ 0.0003…    ┆   ┆ 0.044652…  ┆ -11.8842…  ┆ -0.004…    ┆ [-0.7656… │\n",
       "└──────┴──────────┴────────────┴────────────┴───┴────────────┴────────────┴────────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv(\"./reversalData_minor.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>psi_e</th><th>b_e</th><th>psi_plus</th><th>b_plus</th><th>u_list</th><th>r_list</th><th>k_e_psi_e_list</th><th>k_e_b_e_list</th><th>k_e_psi_plus_list</th><th>k_e_b_plus_list</th><th>heat_flux_psi_e_b_e_list</th><th>heat_flux_psi_e_b_plus_list</th><th>b_e_psi_plus_list</th><th>b_e_b_plus_list</th><th>psi_plus_b_plus_list</th><th>eta_list</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;[-0.011744609035812982, -0.011…</td><td>&quot;[-5.517162102724528, -3.192070…</td><td>&quot;[-0.0016475367466768115, -0.00…</td><td>&quot;[10.73076649947905, 10.7758112…</td><td>&quot;[0.4615413218450946, 0.4686702…</td><td>&quot;[0.6551578220909112, 0.8945764…</td><td>&quot;[0.00013793584140409996, 0.000…</td><td>&quot;[30.43907766773974, 10.1893158…</td><td>&quot;[2.7143773316504123e-06, 5.549…</td><td>&quot;[115.14934966634185, 116.11810…</td><td>&quot;[0.06479691188370344, 0.035800…</td><td>&quot;[-0.1260286571909809, -0.12085…</td><td>&quot;[0.009089727301611367, 0.00751…</td><td>&quot;[-59.203378264111755, -34.3971…</td><td>&quot;[-0.01767933212790023, -0.0253…</td><td>&quot;[[1.0369206541904592], [-1.681…</td></tr><tr><td>&quot;[-0.008147157474100065, -0.008…</td><td>&quot;[-0.832904729882089, 0.1407146…</td><td>&quot;[0.0006807256129005129, 0.0007…</td><td>&quot;[-0.7190970906032216, -0.60951…</td><td>&quot;[0.23703131704960073, 0.234794…</td><td>&quot;[-0.18778048603775238, -0.2053…</td><td>&quot;[6.637617490778455e-05, 6.8758…</td><td>&quot;[0.6937302890599556, 0.0198006…</td><td>&quot;[4.6338736005877887e-07, 5.349…</td><td>&quot;[0.5171006257140178, 0.3715125…</td><td>&quot;[0.006785805995272157, -0.0011…</td><td>&quot;[0.005858597236311648, 0.00505…</td><td>&quot;[-0.0005669795827367211, 0.000…</td><td>&quot;[0.5989393680078723, -0.085768…</td><td>&quot;[-0.0004895078077358536, -0.00…</td><td>&quot;[[1.418213787060706], [0.88278…</td></tr><tr><td>&quot;[-0.003781434990007615, -0.001…</td><td>&quot;[7.429632504232679, 7.61617440…</td><td>&quot;[0.0017201097552821953, 0.0016…</td><td>&quot;[0.26192796713213556, -0.01619…</td><td>&quot;[0.3398928494975913, 0.3384786…</td><td>&quot;[-0.22023434840890527, -0.0752…</td><td>&quot;[1.4299250583653892e-05, 1.712…</td><td>&quot;[55.199439147950756, 58.006112…</td><td>&quot;[2.958777570216974e-06, 2.8808…</td><td>&quot;[0.06860625996597308, 0.000262…</td><td>&quot;[-0.028094672314403355, -0.009…</td><td>&quot;[-0.000990463579775022, 2.1190…</td><td>&quot;[0.012779783348692317, 0.01292…</td><td>&quot;[1.9460285383725033, -0.123315…</td><td>&quot;[0.00045054485144522063, -2.74…</td><td>&quot;[[-0.6679438799496836], [-0.80…</td></tr><tr><td>&quot;[-0.008102021680746106, -0.007…</td><td>&quot;[-13.405475487140006, -12.1429…</td><td>&quot;[-0.007065355563569785, -0.007…</td><td>&quot;[3.432058579711073, 2.92797364…</td><td>&quot;[0.15021463519080008, 0.170829…</td><td>&quot;[1.9382048681079675, 1.8501503…</td><td>&quot;[6.564275531527996e-05, 5.7415…</td><td>&quot;[179.70677303631157, 147.45183…</td><td>&quot;[4.991924923966652e-05, 5.2004…</td><td>&quot;[11.779026094568387, 8.5730296…</td><td>&quot;[0.10861145303751879, 0.092010…</td><td>&quot;[-0.0278066130224098, -0.02218…</td><td>&quot;[0.09471445081536302, 0.087568…</td><td>&quot;[-46.00837716074533, -35.55430…</td><td>&quot;[-0.024248714180659044, -0.021…</td><td>&quot;[[0.20668630436664126], [1.034…</td></tr><tr><td>&quot;[-0.0007052290130828054, -0.00…</td><td>&quot;[7.188510348955567, 7.25531527…</td><td>&quot;[0.01155266108186768, 0.011468…</td><td>&quot;[-0.9665805895951766, 0.236189…</td><td>&quot;[0.1388431307750241, 0.1327890…</td><td>&quot;[-0.2758572865676108, -1.34969…</td><td>&quot;[4.973479608937477e-07, 1.2082…</td><td>&quot;[51.67468103704128, 52.6395997…</td><td>&quot;[0.0001334639780725001, 0.0001…</td><td>&quot;[0.9342780361821591, 0.0557856…</td><td>&quot;[-0.0050695460589294675, -0.02…</td><td>&quot;[0.0006816606752652025, -0.000…</td><td>&quot;[0.08304642374498203, 0.083205…</td><td>&quot;[-6.9482745714045, 1.713632080…</td><td>&quot;[-0.011166577959904913, 0.0027…</td><td>&quot;[[-0.2944195173353409], [0.846…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 16)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ psi_e     ┆ b_e       ┆ psi_plus  ┆ b_plus    ┆ … ┆ b_e_psi_p ┆ b_e_b_plu ┆ psi_plus_ ┆ eta_list │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ lus_list  ┆ s_list    ┆ b_plus_li ┆ ---      │\n",
       "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆ ---       ┆ ---       ┆ st        ┆ str      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ str       ┆ str       ┆ ---       ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ str       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ [-0.01174 ┆ [-5.51716 ┆ [-0.00164 ┆ [10.73076 ┆ … ┆ [0.009089 ┆ [-59.2033 ┆ [-0.01767 ┆ [[1.0369 │\n",
       "│ 460903581 ┆ 210272452 ┆ 753674667 ┆ 649947905 ┆   ┆ 727301611 ┆ 782641117 ┆ 933212790 ┆ 20654190 │\n",
       "│ 2982,     ┆ 8, -3.192 ┆ 68115,    ┆ , 10.7758 ┆   ┆ 367,      ┆ 55,       ┆ 023,      ┆ 4592],   │\n",
       "│ -0.011…   ┆ 070…      ┆ -0.00…    ┆ 112…      ┆   ┆ 0.00751…  ┆ -34.3971… ┆ -0.0253…  ┆ [-1.681… │\n",
       "│ [-0.00814 ┆ [-0.83290 ┆ [0.000680 ┆ [-0.71909 ┆ … ┆ [-0.00056 ┆ [0.598939 ┆ [-0.00048 ┆ [[1.4182 │\n",
       "│ 715747410 ┆ 472988208 ┆ 725612900 ┆ 709060322 ┆   ┆ 697958273 ┆ 368007872 ┆ 950780773 ┆ 13787060 │\n",
       "│ 0065,     ┆ 9, 0.1407 ┆ 5129,     ┆ 16,       ┆   ┆ 67211,    ┆ 3, -0.085 ┆ 58536,    ┆ 706],    │\n",
       "│ -0.008…   ┆ 146…      ┆ 0.0007…   ┆ -0.60951… ┆   ┆ 0.000…    ┆ 768…      ┆ -0.00…    ┆ [0.88278 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ …        │\n",
       "│ [-0.00378 ┆ [7.429632 ┆ [0.001720 ┆ [0.261927 ┆ … ┆ [0.012779 ┆ [1.946028 ┆ [0.000450 ┆ [[-0.667 │\n",
       "│ 143499000 ┆ 504232679 ┆ 109755282 ┆ 967132135 ┆   ┆ 783348692 ┆ 538372503 ┆ 544851445 ┆ 94387994 │\n",
       "│ 7615,     ┆ , 7.61617 ┆ 1953,     ┆ 56,       ┆   ┆ 317,      ┆ 3, -0.123 ┆ 22063,    ┆ 96836],  │\n",
       "│ -0.001…   ┆ 440…      ┆ 0.0016…   ┆ -0.01619… ┆   ┆ 0.01292…  ┆ 315…      ┆ -2.74…    ┆ [-0.80…  │\n",
       "│ [-0.00810 ┆ [-13.4054 ┆ [-0.00706 ┆ [3.432058 ┆ … ┆ [0.094714 ┆ [-46.0083 ┆ [-0.02424 ┆ [[0.2066 │\n",
       "│ 202168074 ┆ 754871400 ┆ 535556356 ┆ 579711073 ┆   ┆ 450815363 ┆ 771607453 ┆ 871418065 ┆ 86304366 │\n",
       "│ 6106,     ┆ 06,       ┆ 9785,     ┆ , 2.92797 ┆   ┆ 02,       ┆ 3, -35.55 ┆ 9044,     ┆ 64126],  │\n",
       "│ -0.007…   ┆ -12.1429… ┆ -0.007…   ┆ 364…      ┆   ┆ 0.087568… ┆ 430…      ┆ -0.021…   ┆ [1.034…  │\n",
       "│ [-0.00070 ┆ [7.188510 ┆ [0.011552 ┆ [-0.96658 ┆ … ┆ [0.083046 ┆ [-6.94827 ┆ [-0.01116 ┆ [[-0.294 │\n",
       "│ 522901308 ┆ 348955567 ┆ 661081867 ┆ 058959517 ┆   ┆ 423744982 ┆ 45714045, ┆ 657795990 ┆ 41951733 │\n",
       "│ 28054,    ┆ , 7.25531 ┆ 68,       ┆ 66,       ┆   ┆ 03,       ┆ 1.7136320 ┆ 4913,     ┆ 53409],  │\n",
       "│ -0.00…    ┆ 527…      ┆ 0.011468… ┆ 0.236189… ┆   ┆ 0.083205… ┆ 80…       ┆ 0.0027…   ┆ [0.846…  │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop([\"id\", \"eps\", \"n_0_squared\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>psi_e</th><th>b_e</th><th>psi_plus</th><th>b_plus</th><th>u_list</th><th>r_list</th><th>k_e_psi_e_list</th><th>k_e_b_e_list</th><th>k_e_psi_plus_list</th><th>k_e_b_plus_list</th><th>heat_flux_psi_e_b_e_list</th><th>heat_flux_psi_e_b_plus_list</th><th>b_e_psi_plus_list</th><th>b_e_b_plus_list</th><th>psi_plus_b_plus_list</th><th>eta_list</th></tr><tr><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[list[f64]]</td></tr></thead><tbody><tr><td>[-0.011745, -0.011215, … -0.011728]</td><td>[-5.517162, -3.192071, … 6.209561]</td><td>[-0.001648, -0.002356, … 0.000093]</td><td>[10.730766, 10.775811, … 2.362047]</td><td>[0.461541, 0.46867, … -0.278246]</td><td>[0.655158, 0.894576, … -0.036835]</td><td>[0.000138, 0.000126, … 0.000138]</td><td>[30.439078, 10.189316, … 38.558648]</td><td>[0.000003, 0.000006, … 8.6049e-9]</td><td>[115.14935, 116.118109, … 5.579266]</td><td>[0.064797, 0.0358, … -0.072824]</td><td>[-0.126029, -0.120854, … -0.027701]</td><td>[0.00909, 0.00752, … 0.000576]</td><td>[-59.203378, -34.397152, … 14.667275]</td><td>[-0.017679, -0.025385, … 0.000219]</td><td>[[1.036921], [-1.681305], … [-0.782573]]</td></tr><tr><td>[-0.008147, -0.008292, … -0.004894]</td><td>[-0.832905, 0.140715, … -8.502144]</td><td>[0.000681, 0.000731, … 0.006186]</td><td>[-0.719097, -0.609518, … -5.884718]</td><td>[0.237031, 0.234794, … -0.347979]</td><td>[-0.18778, -0.20534, … -1.025089]</td><td>[0.000066, 0.000069, … 0.000024]</td><td>[0.69373, 0.019801, … 72.28645]</td><td>[4.6339e-7, 5.3490e-7, … 0.000038]</td><td>[0.517101, 0.371513, … 34.629909]</td><td>[0.006786, -0.001167, … 0.041612]</td><td>[0.005859, 0.005054, … 0.028802]</td><td>[-0.000567, 0.000103, … -0.052593]</td><td>[0.598939, -0.085768, … 50.032721]</td><td>[-0.00049, -0.000446, … -0.036402]</td><td>[[1.418214], [0.882783], … [-0.412105]]</td></tr><tr><td>[-0.003781, -0.001309, … 0.004499]</td><td>[7.429633, 7.616174, … 0.77588]</td><td>[0.00172, 0.001697, … 0.001258]</td><td>[0.261928, -0.016191, … -5.434772]</td><td>[0.339893, 0.338479, … -0.232213]</td><td>[-0.220234, -0.075214, … 0.191679]</td><td>[0.000014, 0.000002, … 0.00002]</td><td>[55.199439, 58.006113, … 0.60199]</td><td>[0.000003, 0.000003, … 0.000002]</td><td>[0.068606, 0.000262, … 29.53675]</td><td>[-0.028095, -0.009968, … 0.003491]</td><td>[-0.00099, 0.000021, … -0.024451]</td><td>[0.01278, 0.012927, … 0.000976]</td><td>[1.946029, -0.123316, … -4.216733]</td><td>[0.000451, -0.000027, … -0.006839]</td><td>[[-0.667944], [-0.804122], … [0.820629]]</td></tr><tr><td>[-0.008102, -0.007577, … 0.016667]</td><td>[-13.405475, -12.142975, … 11.825626]</td><td>[-0.007065, -0.007211, … -0.00118]</td><td>[3.432059, 2.927974, … 3.577291]</td><td>[0.150215, 0.17083, … -0.121867]</td><td>[1.938205, 1.85015, … -0.665716]</td><td>[0.000066, 0.000057, … 0.000278]</td><td>[179.706773, 147.451835, … 139.845423]</td><td>[0.00005, 0.000052, … 0.000001]</td><td>[11.779026, 8.57303, … 12.797011]</td><td>[0.108611, 0.092011, … 0.1971]</td><td>[-0.027807, -0.022186, … 0.059623]</td><td>[0.094714, 0.087568, … -0.01395]</td><td>[-46.008377, -35.55431, … 42.303705]</td><td>[-0.024249, -0.021115, … -0.00422]</td><td>[[0.206686], [1.034311], … [-2.368951]]</td></tr><tr><td>[-0.000705, -0.003476, … 0.004359]</td><td>[7.18851, 7.255315, … -1.118738]</td><td>[0.011553, 0.011468, … 0.001921]</td><td>[-0.966581, 0.23619, … 0.077109]</td><td>[0.138843, 0.132789, … -0.237516]</td><td>[-0.275857, -1.349696, … 0.283456]</td><td>[4.9735e-7, 0.000012, … 0.000019]</td><td>[51.674681, 52.6396, … 1.251575]</td><td>[0.000133, 0.000132, … 0.000004]</td><td>[0.934278, 0.055786, … 0.005946]</td><td>[-0.00507, -0.025219, … -0.004877]</td><td>[0.000682, -0.000821, … 0.000336]</td><td>[0.083046, 0.083205, … -0.002149]</td><td>[-6.948275, 1.713632, … -0.086265]</td><td>[-0.011167, 0.002709, … 0.000148]</td><td>[[-0.29442], [0.846611], … [0.437807]]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 16)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ psi_e     ┆ b_e       ┆ psi_plus  ┆ b_plus    ┆ … ┆ b_e_psi_p ┆ b_e_b_plu ┆ psi_plus_ ┆ eta_list │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ lus_list  ┆ s_list    ┆ b_plus_li ┆ ---      │\n",
       "│ list[f64] ┆ list[f64] ┆ list[f64] ┆ list[f64] ┆   ┆ ---       ┆ ---       ┆ st        ┆ list[lis │\n",
       "│           ┆           ┆           ┆           ┆   ┆ list[f64] ┆ list[f64] ┆ ---       ┆ t[f64]]  │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ list[f64] ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ [-0.01174 ┆ [-5.51716 ┆ [-0.00164 ┆ [10.73076 ┆ … ┆ [0.00909, ┆ [-59.2033 ┆ [-0.01767 ┆ [[1.0369 │\n",
       "│ 5, -0.011 ┆ 2, -3.192 ┆ 8, -0.002 ┆ 6, 10.775 ┆   ┆ 0.00752,  ┆ 78, -34.3 ┆ 9, -0.025 ┆ 21], [-1 │\n",
       "│ 215, …    ┆ 071, …    ┆ 356, …    ┆ 811, …    ┆   ┆ …         ┆ 97152, …  ┆ 385, …    ┆ .681305] │\n",
       "│ -0.01…    ┆ 6.209…    ┆ 0.000…    ┆ 2.362…    ┆   ┆ 0.000576] ┆ 14.…      ┆ 0.000…    ┆ , … [-…  │\n",
       "│ [-0.00814 ┆ [-0.83290 ┆ [0.000681 ┆ [-0.71909 ┆ … ┆ [-0.00056 ┆ [0.598939 ┆ [-0.00049 ┆ [[1.4182 │\n",
       "│ 7, -0.008 ┆ 5,        ┆ ,         ┆ 7, -0.609 ┆   ┆ 7,        ┆ , -0.0857 ┆ , -0.0004 ┆ 14], [0. │\n",
       "│ 292, …    ┆ 0.140715, ┆ 0.000731, ┆ 518, …    ┆   ┆ 0.000103, ┆ 68, …     ┆ 46, …     ┆ 882783], │\n",
       "│ -0.00…    ┆ … -8.502… ┆ …         ┆ -5.88…    ┆   ┆ … -0.052… ┆ 50.032…   ┆ -0.036…   ┆ … [-0…   │\n",
       "│           ┆           ┆ 0.00618…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ [-0.00378 ┆ [7.429633 ┆ [0.00172, ┆ [0.261928 ┆ … ┆ [0.01278, ┆ [1.946029 ┆ [0.000451 ┆ [[-0.667 │\n",
       "│ 1, -0.001 ┆ ,         ┆ 0.001697, ┆ , -0.0161 ┆   ┆ 0.012927, ┆ , -0.1233 ┆ , -0.0000 ┆ 944],    │\n",
       "│ 309, …    ┆ 7.616174, ┆ …         ┆ 91, …     ┆   ┆ …         ┆ 16, …     ┆ 27, …     ┆ [-0.8041 │\n",
       "│ 0.004…    ┆ …         ┆ 0.001258… ┆ -5.434…   ┆   ┆ 0.000976… ┆ -4.216…   ┆ -0.006…   ┆ 22], …   │\n",
       "│           ┆ 0.77588…  ┆           ┆           ┆   ┆           ┆           ┆           ┆ […       │\n",
       "│ [-0.00810 ┆ [-13.4054 ┆ [-0.00706 ┆ [3.432059 ┆ … ┆ [0.094714 ┆ [-46.0083 ┆ [-0.02424 ┆ [[0.2066 │\n",
       "│ 2, -0.007 ┆ 75, -12.1 ┆ 5, -0.007 ┆ ,         ┆   ┆ ,         ┆ 77, -35.5 ┆ 9, -0.021 ┆ 86], [1. │\n",
       "│ 577, …    ┆ 42975, …  ┆ 211, …    ┆ 2.927974, ┆   ┆ 0.087568, ┆ 5431, …   ┆ 115, …    ┆ 034311], │\n",
       "│ 0.016…    ┆ 11.…      ┆ -0.00…    ┆ …         ┆   ┆ …         ┆ 42.3…     ┆ -0.00…    ┆ … [-2…   │\n",
       "│           ┆           ┆           ┆ 3.57729…  ┆   ┆ -0.0139…  ┆           ┆           ┆          │\n",
       "│ [-0.00070 ┆ [7.18851, ┆ [0.011553 ┆ [-0.96658 ┆ … ┆ [0.083046 ┆ [-6.94827 ┆ [-0.01116 ┆ [[-0.294 │\n",
       "│ 5, -0.003 ┆ 7.255315, ┆ ,         ┆ 1,        ┆   ┆ ,         ┆ 5,        ┆ 7,        ┆ 42], [0. │\n",
       "│ 476, …    ┆ …         ┆ 0.011468, ┆ 0.23619,  ┆   ┆ 0.083205, ┆ 1.713632, ┆ 0.002709, ┆ 846611], │\n",
       "│ 0.004…    ┆ -1.11873… ┆ …         ┆ …         ┆   ┆ …         ┆ … -0.086… ┆ … 0.0001… ┆ … [0.…   │\n",
       "│           ┆           ┆ 0.00192…  ┆ 0.07710…  ┆   ┆ -0.0021…  ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.select(\n",
    "    pl.col(\"*\").str.json_decode()\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>psi_e</th><th>b_e</th><th>psi_plus</th><th>b_plus</th><th>u_list</th><th>r_list</th><th>k_e_psi_e_list</th><th>k_e_b_e_list</th><th>k_e_psi_plus_list</th><th>k_e_b_plus_list</th><th>heat_flux_psi_e_b_e_list</th><th>heat_flux_psi_e_b_plus_list</th><th>b_e_psi_plus_list</th><th>b_e_b_plus_list</th><th>psi_plus_b_plus_list</th><th>eta_list</th></tr><tr><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td></tr></thead><tbody><tr><td>[-0.011745, -0.011215, … -0.011728]</td><td>[-5.517162, -3.192071, … 6.209561]</td><td>[-0.001648, -0.002356, … 0.000093]</td><td>[10.730766, 10.775811, … 2.362047]</td><td>[0.461541, 0.46867, … -0.278246]</td><td>[0.655158, 0.894576, … -0.036835]</td><td>[0.000138, 0.000126, … 0.000138]</td><td>[30.439078, 10.189316, … 38.558648]</td><td>[0.000003, 0.000006, … 8.6049e-9]</td><td>[115.14935, 116.118109, … 5.579266]</td><td>[0.064797, 0.0358, … -0.072824]</td><td>[-0.126029, -0.120854, … -0.027701]</td><td>[0.00909, 0.00752, … 0.000576]</td><td>[-59.203378, -34.397152, … 14.667275]</td><td>[-0.017679, -0.025385, … 0.000219]</td><td>[1.036921, -1.681305, … -0.782573]</td></tr><tr><td>[-0.008147, -0.008292, … -0.004894]</td><td>[-0.832905, 0.140715, … -8.502144]</td><td>[0.000681, 0.000731, … 0.006186]</td><td>[-0.719097, -0.609518, … -5.884718]</td><td>[0.237031, 0.234794, … -0.347979]</td><td>[-0.18778, -0.20534, … -1.025089]</td><td>[0.000066, 0.000069, … 0.000024]</td><td>[0.69373, 0.019801, … 72.28645]</td><td>[4.6339e-7, 5.3490e-7, … 0.000038]</td><td>[0.517101, 0.371513, … 34.629909]</td><td>[0.006786, -0.001167, … 0.041612]</td><td>[0.005859, 0.005054, … 0.028802]</td><td>[-0.000567, 0.000103, … -0.052593]</td><td>[0.598939, -0.085768, … 50.032721]</td><td>[-0.00049, -0.000446, … -0.036402]</td><td>[1.418214, 0.882783, … -0.412105]</td></tr><tr><td>[-0.003781, -0.001309, … 0.004499]</td><td>[7.429633, 7.616174, … 0.77588]</td><td>[0.00172, 0.001697, … 0.001258]</td><td>[0.261928, -0.016191, … -5.434772]</td><td>[0.339893, 0.338479, … -0.232213]</td><td>[-0.220234, -0.075214, … 0.191679]</td><td>[0.000014, 0.000002, … 0.00002]</td><td>[55.199439, 58.006113, … 0.60199]</td><td>[0.000003, 0.000003, … 0.000002]</td><td>[0.068606, 0.000262, … 29.53675]</td><td>[-0.028095, -0.009968, … 0.003491]</td><td>[-0.00099, 0.000021, … -0.024451]</td><td>[0.01278, 0.012927, … 0.000976]</td><td>[1.946029, -0.123316, … -4.216733]</td><td>[0.000451, -0.000027, … -0.006839]</td><td>[-0.667944, -0.804122, … 0.820629]</td></tr><tr><td>[-0.008102, -0.007577, … 0.016667]</td><td>[-13.405475, -12.142975, … 11.825626]</td><td>[-0.007065, -0.007211, … -0.00118]</td><td>[3.432059, 2.927974, … 3.577291]</td><td>[0.150215, 0.17083, … -0.121867]</td><td>[1.938205, 1.85015, … -0.665716]</td><td>[0.000066, 0.000057, … 0.000278]</td><td>[179.706773, 147.451835, … 139.845423]</td><td>[0.00005, 0.000052, … 0.000001]</td><td>[11.779026, 8.57303, … 12.797011]</td><td>[0.108611, 0.092011, … 0.1971]</td><td>[-0.027807, -0.022186, … 0.059623]</td><td>[0.094714, 0.087568, … -0.01395]</td><td>[-46.008377, -35.55431, … 42.303705]</td><td>[-0.024249, -0.021115, … -0.00422]</td><td>[0.206686, 1.034311, … -2.368951]</td></tr><tr><td>[-0.000705, -0.003476, … 0.004359]</td><td>[7.18851, 7.255315, … -1.118738]</td><td>[0.011553, 0.011468, … 0.001921]</td><td>[-0.966581, 0.23619, … 0.077109]</td><td>[0.138843, 0.132789, … -0.237516]</td><td>[-0.275857, -1.349696, … 0.283456]</td><td>[4.9735e-7, 0.000012, … 0.000019]</td><td>[51.674681, 52.6396, … 1.251575]</td><td>[0.000133, 0.000132, … 0.000004]</td><td>[0.934278, 0.055786, … 0.005946]</td><td>[-0.00507, -0.025219, … -0.004877]</td><td>[0.000682, -0.000821, … 0.000336]</td><td>[0.083046, 0.083205, … -0.002149]</td><td>[-6.948275, 1.713632, … -0.086265]</td><td>[-0.011167, 0.002709, … 0.000148]</td><td>[-0.29442, 0.846611, … 0.437807]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 16)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ psi_e     ┆ b_e       ┆ psi_plus  ┆ b_plus    ┆ … ┆ b_e_psi_p ┆ b_e_b_plu ┆ psi_plus_ ┆ eta_list │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ lus_list  ┆ s_list    ┆ b_plus_li ┆ ---      │\n",
       "│ list[f64] ┆ list[f64] ┆ list[f64] ┆ list[f64] ┆   ┆ ---       ┆ ---       ┆ st        ┆ list[f64 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ list[f64] ┆ list[f64] ┆ ---       ┆ ]        │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ list[f64] ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ [-0.01174 ┆ [-5.51716 ┆ [-0.00164 ┆ [10.73076 ┆ … ┆ [0.00909, ┆ [-59.2033 ┆ [-0.01767 ┆ [1.03692 │\n",
       "│ 5, -0.011 ┆ 2, -3.192 ┆ 8, -0.002 ┆ 6, 10.775 ┆   ┆ 0.00752,  ┆ 78, -34.3 ┆ 9, -0.025 ┆ 1, -1.68 │\n",
       "│ 215, …    ┆ 071, …    ┆ 356, …    ┆ 811, …    ┆   ┆ …         ┆ 97152, …  ┆ 385, …    ┆ 1305, …  │\n",
       "│ -0.01…    ┆ 6.209…    ┆ 0.000…    ┆ 2.362…    ┆   ┆ 0.000576] ┆ 14.…      ┆ 0.000…    ┆ -0.782…  │\n",
       "│ [-0.00814 ┆ [-0.83290 ┆ [0.000681 ┆ [-0.71909 ┆ … ┆ [-0.00056 ┆ [0.598939 ┆ [-0.00049 ┆ [1.41821 │\n",
       "│ 7, -0.008 ┆ 5,        ┆ ,         ┆ 7, -0.609 ┆   ┆ 7,        ┆ , -0.0857 ┆ , -0.0004 ┆ 4, 0.882 │\n",
       "│ 292, …    ┆ 0.140715, ┆ 0.000731, ┆ 518, …    ┆   ┆ 0.000103, ┆ 68, …     ┆ 46, …     ┆ 783, …   │\n",
       "│ -0.00…    ┆ … -8.502… ┆ …         ┆ -5.88…    ┆   ┆ … -0.052… ┆ 50.032…   ┆ -0.036…   ┆ -0.4121… │\n",
       "│           ┆           ┆ 0.00618…  ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ [-0.00378 ┆ [7.429633 ┆ [0.00172, ┆ [0.261928 ┆ … ┆ [0.01278, ┆ [1.946029 ┆ [0.000451 ┆ [-0.6679 │\n",
       "│ 1, -0.001 ┆ ,         ┆ 0.001697, ┆ , -0.0161 ┆   ┆ 0.012927, ┆ , -0.1233 ┆ , -0.0000 ┆ 44, -0.8 │\n",
       "│ 309, …    ┆ 7.616174, ┆ …         ┆ 91, …     ┆   ┆ …         ┆ 16, …     ┆ 27, …     ┆ 04122, … │\n",
       "│ 0.004…    ┆ …         ┆ 0.001258… ┆ -5.434…   ┆   ┆ 0.000976… ┆ -4.216…   ┆ -0.006…   ┆ 0.820…   │\n",
       "│           ┆ 0.77588…  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ [-0.00810 ┆ [-13.4054 ┆ [-0.00706 ┆ [3.432059 ┆ … ┆ [0.094714 ┆ [-46.0083 ┆ [-0.02424 ┆ [0.20668 │\n",
       "│ 2, -0.007 ┆ 75, -12.1 ┆ 5, -0.007 ┆ ,         ┆   ┆ ,         ┆ 77, -35.5 ┆ 9, -0.021 ┆ 6, 1.034 │\n",
       "│ 577, …    ┆ 42975, …  ┆ 211, …    ┆ 2.927974, ┆   ┆ 0.087568, ┆ 5431, …   ┆ 115, …    ┆ 311, …   │\n",
       "│ 0.016…    ┆ 11.…      ┆ -0.00…    ┆ …         ┆   ┆ …         ┆ 42.3…     ┆ -0.00…    ┆ -2.3689… │\n",
       "│           ┆           ┆           ┆ 3.57729…  ┆   ┆ -0.0139…  ┆           ┆           ┆          │\n",
       "│ [-0.00070 ┆ [7.18851, ┆ [0.011553 ┆ [-0.96658 ┆ … ┆ [0.083046 ┆ [-6.94827 ┆ [-0.01116 ┆ [-0.2944 │\n",
       "│ 5, -0.003 ┆ 7.255315, ┆ ,         ┆ 1,        ┆   ┆ ,         ┆ 5,        ┆ 7,        ┆ 2, 0.846 │\n",
       "│ 476, …    ┆ …         ┆ 0.011468, ┆ 0.23619,  ┆   ┆ 0.083205, ┆ 1.713632, ┆ 0.002709, ┆ 611, …   │\n",
       "│ 0.004…    ┆ -1.11873… ┆ …         ┆ …         ┆   ┆ …         ┆ … -0.086… ┆ … 0.0001… ┆ 0.43780… │\n",
       "│           ┆           ┆ 0.00192…  ┆ 0.07710…  ┆   ┆ -0.0021…  ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.with_columns(\n",
    "    eta_list = pl.col(\"eta_list\").list.eval(pl.element().flatten(), parallel = True)\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.regression import R2Score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_val = 0\n",
    "\n",
    "torch.manual_seed(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 16)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>psi_e</th><th>b_e</th><th>psi_plus</th><th>b_plus</th><th>u_list</th><th>r_list</th><th>k_e_psi_e_list</th><th>k_e_b_e_list</th><th>k_e_psi_plus_list</th><th>k_e_b_plus_list</th><th>heat_flux_psi_e_b_e_list</th><th>heat_flux_psi_e_b_plus_list</th><th>b_e_psi_plus_list</th><th>b_e_b_plus_list</th><th>psi_plus_b_plus_list</th><th>eta_list</th></tr><tr><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td><td>list[f64]</td></tr></thead><tbody><tr><td>[-0.005793, -0.004183, … -0.002956]</td><td>[-5.107394, -4.225599, … 5.978745]</td><td>[-0.003674, -0.003845, … 0.003528]</td><td>[3.18727, 2.998706, … -2.897284]</td><td>[0.328583, 0.33484, … -0.445908]</td><td>[0.720642, 0.544571, … -0.353192]</td><td>[0.000034, 0.000017, … 0.000009]</td><td>[26.085471, 17.855691, … 35.745387]</td><td>[0.000013, 0.000015, … 0.000012]</td><td>[10.158692, 8.992235, … 8.394254]</td><td>[0.029587, 0.017676, … -0.017676]</td><td>[-0.018464, -0.012544, … 0.008566]</td><td>[0.018765, 0.016247, … 0.021095]</td><td>[-16.278645, -12.671329, … -17.32212]</td><td>[-0.01171, -0.01153, … -0.010223]</td><td>[0.511849, -0.243375, … 0.508325]</td></tr><tr><td>[0.021404, 0.021093, … 0.000991]</td><td>[-9.237605, -11.516815, … 5.765189]</td><td>[-0.000833, -0.001172, … 0.001094]</td><td>[4.428155, 4.733523, … -3.332383]</td><td>[0.245995, 0.238335, … -0.401837]</td><td>[-0.603414, -0.837105, … 0.036702]</td><td>[0.000458, 0.000445, … 9.8238e-7]</td><td>[85.333351, 132.637025, … 33.237406]</td><td>[6.9326e-7, 0.000001, … 0.000001]</td><td>[19.60856, 22.406236, … 11.104776]</td><td>[-0.197722, -0.24293, … 0.005714]</td><td>[0.094781, 0.099846, … -0.003303]</td><td>[0.007691, 0.013499, … 0.006305]</td><td>[-40.905552, -54.515103, … -19.211818]</td><td>[-0.003687, -0.005548, … -0.003644]</td><td>[0.063621, 0.334498, … 0.674818]</td></tr><tr><td>[-0.015647, -0.013076, … -0.00386]</td><td>[-2.369924, -0.49071, … 7.107985]</td><td>[-0.003052, -0.003181, … -0.003636]</td><td>[2.772283, 2.464133, … -6.293166]</td><td>[0.320847, 0.335211, … -0.332921]</td><td>[1.616712, 1.408141, … 0.475131]</td><td>[0.000245, 0.000171, … 0.000015]</td><td>[5.61654, 0.240796, … 50.523454]</td><td>[0.000009, 0.00001, … 0.000013]</td><td>[7.685551, 6.07195, … 39.603939]</td><td>[0.037081, 0.006417, … -0.027434]</td><td>[-0.043377, -0.032221, … 0.024289]</td><td>[0.007232, 0.001561, … -0.025843]</td><td>[-6.570099, -1.209175, … -44.731731]</td><td>[-0.00846, -0.007837, … 0.02288]</td><td>[-0.20652, 0.336207, … -1.636342]</td></tr><tr><td>[-0.011174, -0.009445, … 0.001876]</td><td>[13.150369, 14.2002, … 16.964786]</td><td>[-0.000778, -0.000786, … -0.003702]</td><td>[0.488479, 0.083618, … -1.323443]</td><td>[0.118517, 0.120958, … -0.26322]</td><td>[0.294501, 0.251263, … -0.235176]</td><td>[0.000125, 0.000089, … 0.000004]</td><td>[172.932208, 201.645674, … 287.803958]</td><td>[6.0592e-7, 6.1736e-7, … 0.000014]</td><td>[0.238612, 0.006992, … 1.751502]</td><td>[-0.146942, -0.134116, … 0.031832]</td><td>[-0.005458, -0.00079, … -0.002483]</td><td>[-0.010236, -0.011157, … -0.062799]</td><td>[6.423679, 1.187386, … -22.451931]</td><td>[-0.00038, -0.000066, … 0.004899]</td><td>[-1.079961, 0.697676, … 0.485542]</td></tr><tr><td>[-0.002717, -0.00317, … -0.019602]</td><td>[3.219618, 3.510212, … 11.002655]</td><td>[0.001369, 0.001422, … -0.002804]</td><td>[-0.827359, -0.89963, … 3.683177]</td><td>[0.396805, 0.394857, … -0.212469]</td><td>[-0.125937, -0.152674, … 1.861283]</td><td>[0.000007, 0.00001, … 0.000384]</td><td>[10.365939, 12.32159, … 121.058415]</td><td>[0.000002, 0.000002, … 0.000008]</td><td>[0.684523, 0.809334, … 13.565793]</td><td>[-0.008748, -0.011128, … -0.215678]</td><td>[0.002248, 0.002852, … -0.072199]</td><td>[0.004407, 0.004993, … -0.030855]</td><td>[-2.663781, -3.157892, … 40.524725]</td><td>[-0.001133, -0.00128, … -0.010329]</td><td>[-0.240056, 2.203021, … 0.820105]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 16)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ psi_e     ┆ b_e       ┆ psi_plus  ┆ b_plus    ┆ … ┆ b_e_psi_p ┆ b_e_b_plu ┆ psi_plus_ ┆ eta_list │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ lus_list  ┆ s_list    ┆ b_plus_li ┆ ---      │\n",
       "│ list[f64] ┆ list[f64] ┆ list[f64] ┆ list[f64] ┆   ┆ ---       ┆ ---       ┆ st        ┆ list[f64 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ list[f64] ┆ list[f64] ┆ ---       ┆ ]        │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ list[f64] ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ [-0.00579 ┆ [-5.10739 ┆ [-0.00367 ┆ [3.18727, ┆ … ┆ [0.018765 ┆ [-16.2786 ┆ [-0.01171 ┆ [0.51184 │\n",
       "│ 3, -0.004 ┆ 4, -4.225 ┆ 4, -0.003 ┆ 2.998706, ┆   ┆ ,         ┆ 45, -12.6 ┆ ,         ┆ 9, -0.24 │\n",
       "│ 183, …    ┆ 599, …    ┆ 845, …    ┆ …         ┆   ┆ 0.016247, ┆ 71329, …  ┆ -0.01153, ┆ 3375, …  │\n",
       "│ -0.00…    ┆ 5.978…    ┆ 0.003…    ┆ -2.89728… ┆   ┆ …         ┆ -17…      ┆ …         ┆ 0.5083…  │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 0.02109…  ┆           ┆ -0.0102…  ┆          │\n",
       "│ [0.021404 ┆ [-9.23760 ┆ [-0.00083 ┆ [4.428155 ┆ … ┆ [0.007691 ┆ [-40.9055 ┆ [-0.00368 ┆ [0.06362 │\n",
       "│ ,         ┆ 5, -11.51 ┆ 3, -0.001 ┆ ,         ┆   ┆ ,         ┆ 52, -54.5 ┆ 7, -0.005 ┆ 1, 0.334 │\n",
       "│ 0.021093, ┆ 6815, …   ┆ 172, …    ┆ 4.733523, ┆   ┆ 0.013499, ┆ 15103, …  ┆ 548, …    ┆ 498, …   │\n",
       "│ …         ┆ 5.76…     ┆ 0.001…    ┆ …         ┆   ┆ …         ┆ -19…      ┆ -0.00…    ┆ 0.67481… │\n",
       "│ 0.00099…  ┆           ┆           ┆ -3.3323…  ┆   ┆ 0.00630…  ┆           ┆           ┆          │\n",
       "│ [-0.01564 ┆ [-2.36992 ┆ [-0.00305 ┆ [2.772283 ┆ … ┆ [0.007232 ┆ [-6.57009 ┆ [-0.00846 ┆ [-0.2065 │\n",
       "│ 7, -0.013 ┆ 4,        ┆ 2, -0.003 ┆ ,         ┆   ┆ ,         ┆ 9, -1.209 ┆ , -0.0078 ┆ 2, 0.336 │\n",
       "│ 076, …    ┆ -0.49071, ┆ 181, …    ┆ 2.464133, ┆   ┆ 0.001561, ┆ 175, …    ┆ 37, …     ┆ 207, …   │\n",
       "│ -0.00…    ┆ … 7.1079… ┆ -0.00…    ┆ …         ┆   ┆ …         ┆ -44.7…    ┆ 0.0228…   ┆ -1.6363… │\n",
       "│           ┆           ┆           ┆ -6.2931…  ┆   ┆ -0.0258…  ┆           ┆           ┆          │\n",
       "│ [-0.01117 ┆ [13.15036 ┆ [-0.00077 ┆ [0.488479 ┆ … ┆ [-0.01023 ┆ [6.423679 ┆ [-0.00038 ┆ [-1.0799 │\n",
       "│ 4, -0.009 ┆ 9,        ┆ 8, -0.000 ┆ ,         ┆   ┆ 6, -0.011 ┆ ,         ┆ , -0.0000 ┆ 61, 0.69 │\n",
       "│ 445, …    ┆ 14.2002,  ┆ 786, …    ┆ 0.083618, ┆   ┆ 157, …    ┆ 1.187386, ┆ 66, …     ┆ 7676, …  │\n",
       "│ 0.001…    ┆ …         ┆ -0.00…    ┆ …         ┆   ┆ -0.06…    ┆ …         ┆ 0.0048…   ┆ 0.4855…  │\n",
       "│           ┆ 16.9647…  ┆           ┆ -1.3234…  ┆   ┆           ┆ -22.451…  ┆           ┆          │\n",
       "│ [-0.00271 ┆ [3.219618 ┆ [0.001369 ┆ [-0.82735 ┆ … ┆ [0.004407 ┆ [-2.66378 ┆ [-0.00113 ┆ [-0.2400 │\n",
       "│ 7,        ┆ ,         ┆ ,         ┆ 9,        ┆   ┆ ,         ┆ 1, -3.157 ┆ 3,        ┆ 56, 2.20 │\n",
       "│ -0.00317, ┆ 3.510212, ┆ 0.001422, ┆ -0.89963, ┆   ┆ 0.004993, ┆ 892, …    ┆ -0.00128, ┆ 3021, …  │\n",
       "│ … -0.019… ┆ …         ┆ …         ┆ … 3.6831… ┆   ┆ …         ┆ 40.52…    ┆ … -0.010… ┆ 0.8201…  │\n",
       "│           ┆ 11.0026…  ┆ -0.0028…  ┆           ┆   ┆ -0.0308…  ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(n = len(df), with_replacement = False, shuffle = True, seed = seed_val)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Val-Test Split (70-20-10)\n",
    "\n",
    "1. Split based on 1500 time-series samples (Vertically)\n",
    "2. Split each sample to train-val-test (Horizontally)\n",
    "\n",
    "Vertically for now!\n",
    "\n",
    "#### How about 5-Fold Cross-Validation? Stratified 5F?\n",
    "\n",
    "Just a single split for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:int(len(df) * 0.7)]\n",
    "df_val = df[int(len(df) * 0.7):int(len(df) * 0.9)]\n",
    "df_test = df[int(len(df) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape: (1050, 16)\n",
      "df_val shape: (300, 16)\n",
      "df_test shape: (150, 16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"df_train shape: {df_train.shape}\")\n",
    "print(f\"df_val shape: {df_val.shape}\")\n",
    "print(f\"df_test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Demand Data Loading (No RAM Issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, input_df, label_df, num_single_sample_timesteps, stride, input_window_length, label_window_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_df = input_df  # Type: Numpy, Shape: Number of time-series, Number of time-steps, Number of input features\n",
    "        self.label_df = label_df  # Type: Numpy, Shape: Number of time-series, Number of time-steps, Number of label features\n",
    "        self.num_single_sample_timesteps = num_single_sample_timesteps\n",
    "        self.stride = stride\n",
    "        self.input_window_length = input_window_length\n",
    "        self.label_window_length = label_window_length\n",
    "\n",
    "        self.valid_length = self.input_window_length + self.label_window_length\n",
    "        \n",
    "        self.window_indices = []\n",
    "        for time_series_idx in range(self.input_df.shape[0]):\n",
    "            for input_window_start_idx in range(0, self.input_df.shape[1] - self.valid_length + 1, self.stride):\n",
    "                self.window_indices.append((time_series_idx, input_window_start_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.window_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        time_series_idx, input_window_start_idx = self.window_indices[index]\n",
    "\n",
    "        label_window_start_idx = input_window_start_idx + self.input_window_length\n",
    "        input_window = self.input_df[time_series_idx, input_window_start_idx: label_window_start_idx, :]\n",
    "        label_window = self.label_df[time_series_idx, label_window_start_idx: label_window_start_idx + self.label_window_length, :]\n",
    "\n",
    "        input_window_mean = input_window.mean(axis = 0)\n",
    "        input_window_std = input_window.std(axis = 0)\n",
    "        input_window_std[input_window_std == 0] = 10 ** -8\n",
    "        input_window = (input_window - input_window_mean) / input_window_std\n",
    "\n",
    "        return torch.tensor(input_window, dtype = torch.float), torch.tensor(label_window, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.explode(\"*\")\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import polars as pl\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# class CausallyNormalizedTimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, csv_path, input_len=200, label_len=100, stride=1):\n",
    "#         self.input_len = input_len\n",
    "#         self.label_len = label_len\n",
    "#         self.total_len = input_len + label_len\n",
    "#         self.stride = stride\n",
    "\n",
    "#         # Load dataset from CSV (assume shape: [num_samples, time_steps * features])\n",
    "#         df = pl.read_csv(csv_path)\n",
    "#         raw_np = df.to_numpy()\n",
    "\n",
    "#         # Infer shape\n",
    "#         num_samples, total_cols = raw_np.shape\n",
    "#         if total_cols % 1000 != 0:\n",
    "#             raise ValueError(\"Expected time series length of 1000 per sample\")\n",
    "#         self.sequence_len = 1000\n",
    "#         self.num_features = total_cols // self.sequence_len\n",
    "\n",
    "#         # Reshape to (N, T, F)\n",
    "#         self.data = raw_np.reshape(num_samples, self.sequence_len, self.num_features)\n",
    "#         self.num_samples = num_samples\n",
    "\n",
    "#         assert self.sequence_len >= self.total_len, \"Time series too short for given input + label length\"\n",
    "\n",
    "#         # Compute mean/std using only the first input_len steps of each event\n",
    "#         first_input = self.data[:, :self.input_len, :]  # shape: (N, input_len, F)\n",
    "#         self.row_means = first_input.mean(axis=1, keepdims=True)  # shape: (N, 1, F)\n",
    "#         self.row_stds = first_input.std(axis=1, keepdims=True) + 1e-8\n",
    "\n",
    "#         # Precompute valid window positions\n",
    "#         self.window_indices = []\n",
    "#         max_start = self.sequence_len - self.total_len\n",
    "#         for sample_idx in range(self.num_samples):\n",
    "#             for t_start in range(0, max_start + 1, self.stride):\n",
    "#                 self.window_indices.append((sample_idx, t_start))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.window_indices)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sample_idx, t_start = self.window_indices[idx]\n",
    "\n",
    "#         full_sequence = self.data[sample_idx]  # (T, F)\n",
    "#         mean = self.row_means[sample_idx]      # (1, F)\n",
    "#         std = self.row_stds[sample_idx]        # (1, F)\n",
    "\n",
    "#         norm_sequence = (full_sequence - mean) / std  # (T, F)\n",
    "\n",
    "#         x = norm_sequence[t_start: t_start + self.input_len]  # (input_len, F)\n",
    "#         y = norm_sequence[t_start + self.input_len: t_start + self.total_len]  # (label_len, F)\n",
    "\n",
    "#         return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# dataset = CausallyNormalizedTimeSeriesDataset(\n",
    "#     csv_path=\"your_multivariate_dataset.csv\",\n",
    "#     input_len=200,\n",
    "#     label_len=100,\n",
    "#     stride=1\n",
    "# )\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# for x, y in loader:\n",
    "#     print(\"Input shape:\", x.shape)   # (32, 200, F)\n",
    "#     print(\"Label shape:\", y.shape)   # (32, 100, F)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All-in-one-go Data Windowing (Possible RAM overflow)\n",
    "\n",
    "Must be super modular since we will run with diverse set of windows!\n",
    "\n",
    "There is a helper function in Keras but I'm going full PyTorch! (`timeseries_dataset_from_array`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Important! Moving average must be implemented!\n",
    "\n",
    "During training you shouldn't have access to future tokens for normalization! --> Data Leakage\n",
    "\n",
    "For now just z-score on the whole train set for each row separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All-in-one-go Data Windowing\n",
    "#\n",
    "# df_train = df_train.select(\n",
    "#     (pl.col(\"*\") - pl.col(\"*\").list.mean()) / pl.col(\"*\").list.std()\n",
    "# )\n",
    "\n",
    "# df_val = df_val.select(\n",
    "#     (pl.col(\"*\") - pl.col(\"*\").list.mean()) / pl.col(\"*\").list.std()\n",
    "# )\n",
    "\n",
    "# df_test = df_test.select(\n",
    "#     (pl.col(\"*\") - pl.col(\"*\").list.mean()) / pl.col(\"*\").list.std()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I am doing above is global normalization where I calculate mean and std across all time-series. There are other options:\n",
    "\n",
    "1. Calculate mean and std per series (num_single_sample_timesteps) and apply to windows --> Temporal leakage! --> Knowing about future distribution\n",
    "2. Calculate mean and std per window (input_window_length) and apply to each window --> Locally aware might lose global context?\n",
    "3. Calculate mean and std based on pervious steps of the current window, normalize the current window with that mean and std\n",
    "\n",
    "I am implementing step 2 for now in the custom dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a violin plot like the one here (https://www.tensorflow.org/tutorials/structured_data/time_series#normalize_the_data) is a good idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset whole!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one-go Data Windowing\n",
    "#\n",
    "# def createWindowedDataframe(dataframe, stride, input_features, input_window_length, label_features, label_window_length):\n",
    "#     \"\"\"\n",
    "#         Creates a windowed time-series dataset suitable for neural network (transformer) input\n",
    "\n",
    "#         dataframe: In time-series format where columns are features and rows are time-series steps (Polars exploded format!)\n",
    "#         stride: How many timesteps will each window move by. Equal for both input and label windows\n",
    "#         input_features: Names of the input features in a list\n",
    "#         input_window_length: Input window size in timesteps\n",
    "#         label_features: Names of the label features in a list\n",
    "#         label_window_length: label window size in timesteps\n",
    "#     \"\"\"\n",
    "\n",
    "#     input_df = dataframe.select(\n",
    "#         pl.col(input_features)\n",
    "#     )\n",
    "#     input_df = input_df.select(\n",
    "#         numbers = pl.concat_list(\"*\")   # List of all features\n",
    "#     )\n",
    "#     input_df = input_df[:-label_window_length].with_row_index(\"index\").with_columns(    # Exluding the ending label window\n",
    "#         pl.col(\"index\").cast(pl.Int64)\n",
    "#     )\n",
    "#     input_df = input_df.group_by_dynamic(   # index is considered for grouping\n",
    "#         index_column = \"index\",\n",
    "#         period = f\"{input_window_length}i\",\n",
    "#         every = f\"{stride}i\",\n",
    "#         closed = \"left\"\n",
    "#     ).agg(\n",
    "#         pl.col(\"numbers\").alias(\"X\"),\n",
    "#         pl.len().alias(\"seq_len\")\n",
    "#     )\n",
    "#     input_df = input_df.filter(\n",
    "#         pl.col(\"seq_len\") == input_window_length\n",
    "#     )\n",
    "#     input_df = input_df.select(\n",
    "#         pl.exclude([\"index\", \"seq_len\"])\n",
    "#     )\n",
    "\n",
    "\n",
    "#     label_df = dataframe.select(\n",
    "#         pl.col(label_features)\n",
    "#     )\n",
    "#     label_df = label_df.select(\n",
    "#         numbers = pl.concat_list(\"*\")\n",
    "#     )\n",
    "#     label_df = label_df[input_window_length:].with_row_index(\"index\").with_columns(\n",
    "#         pl.col(\"index\").cast(pl.Int64)\n",
    "#     )\n",
    "#     label_df = label_df.group_by_dynamic(\n",
    "#         index_column = \"index\",\n",
    "#         period = f\"{label_window_length}i\",\n",
    "#         every = f\"{stride}i\",\n",
    "#         closed = \"left\"\n",
    "#     ).agg(\n",
    "#         pl.col(\"numbers\").alias(\"Y\"),\n",
    "#         pl.len().alias(\"seq_len\")\n",
    "#     )\n",
    "#     label_df = label_df.filter(\n",
    "#         (pl.col(\"seq_len\") == label_window_length)\n",
    "#     )\n",
    "#     label_df = label_df.select(\n",
    "#         pl.exclude([\"index\", \"seq_len\"])\n",
    "#     )\n",
    "\n",
    "#     return pl.concat([input_df, label_df], how = \"horizontal\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one-go Data Windowing\n",
    "#\n",
    "# df_train = df_train.explode(\"*\")\n",
    "\n",
    "# timeseries_df_train = pl.DataFrame()\n",
    "\n",
    "# for i in range(0, len(df_train), num_single_sample_timesteps):\n",
    "#     temp_df = createWindowedDataframe(\n",
    "#         dataframe = df_train[i:i + num_single_sample_timesteps],\n",
    "#         stride = window_stride,\n",
    "#         input_features = input_features,\n",
    "#         input_window_length = input_window_length,\n",
    "#         label_features = label_features,\n",
    "#         label_window_length = label_window_length\n",
    "#     )\n",
    "\n",
    "#     if(temp_df.is_empty()):\n",
    "#         timeseries_df_train = temp_df\n",
    "#     else:\n",
    "#         timeseries_df_train = pl.concat([timeseries_df_train, temp_df], how = \"vertical\")\n",
    "\n",
    "# timeseries_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one-go Data Windowing\n",
    "#\n",
    "# df_val = df_val.explode(\"*\")\n",
    "\n",
    "# timeseries_df_val = pl.DataFrame()\n",
    "\n",
    "# for i in range(0, len(df_val), num_single_sample_timesteps):\n",
    "#     temp_df = createWindowedDataframe(\n",
    "#         dataframe = df_val[i:i + num_single_sample_timesteps],\n",
    "#         stride = window_stride,\n",
    "#         input_features = input_features,\n",
    "#         input_window_length = input_window_length,\n",
    "#         label_features = label_features,\n",
    "#         label_window_length = label_window_length\n",
    "#     )\n",
    "\n",
    "#     if(temp_df.is_empty()):\n",
    "#         timeseries_df_val = temp_df\n",
    "#     else:\n",
    "#         timeseries_df_val = pl.concat([timeseries_df_val, temp_df], how = \"vertical\")\n",
    "\n",
    "# timeseries_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one-go Data Windowing\n",
    "#\n",
    "# df_test = df_test.explode(\"*\")\n",
    "\n",
    "# timeseries_df_test = pl.DataFrame()\n",
    "\n",
    "# for i in range(0, len(df_test), num_single_sample_timesteps):\n",
    "#     temp_df = createWindowedDataframe(\n",
    "#         dataframe = df_test[i:i + num_single_sample_timesteps],\n",
    "#         stride = window_stride,\n",
    "#         input_features = input_features,\n",
    "#         input_window_length = input_window_length,\n",
    "#         label_features = label_features,\n",
    "#         label_window_length = label_window_length\n",
    "#     )\n",
    "\n",
    "#     if(temp_df.is_empty()):\n",
    "#         timeseries_df_test = temp_df\n",
    "#     else:\n",
    "#         timeseries_df_test = pl.concat([timeseries_df_test, temp_df], how = \"vertical\")\n",
    "\n",
    "# timeseries_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some visualization here is good to do a sanity-check on the dataset before sending it to the model!!!!!!!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, num_head, num_encoder_layers, num_decoder_layers, positional_encoding_max_len, position_wise_ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_proj = torch.nn.Linear(input_dim, d_model)\n",
    "        self.output_proj = torch.nn.Linear(output_dim, d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model = d_model,\n",
    "            dropout = dropout,\n",
    "            max_len = positional_encoding_max_len\n",
    "        )\n",
    "\n",
    "        self.transformer = torch.nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = num_head,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dropout = dropout,\n",
    "            dim_feedforward = position_wise_ffn_dim,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        self.final_proj = torch.nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask = None):\n",
    "        # encode and decode methods can be used here\n",
    "\n",
    "        src = self.input_proj(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        tgt = self.output_proj(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        if(tgt_mask is None):\n",
    "            tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        decoder_out = self.transformer(src, tgt, tgt_mask = tgt_mask)\n",
    "\n",
    "        return self.final_proj(decoder_out)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        # memory is encoder output\n",
    "\n",
    "        src = self.input_proj(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.transformer.encoder(src)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask = None):\n",
    "        tgt = self.output_proj(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        if(tgt_mask is None):\n",
    "            tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        decoder_out = self.transformer.decoder(tgt, memory, tgt_mask = tgt_mask)\n",
    "\n",
    "        return self.final_proj(decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one-go Data Windowing\n",
    "\n",
    "# df_train = timeseries_df_train\n",
    "# df_val = timeseries_df_val\n",
    "# df_test = timeseries_df_test\n",
    "\n",
    "# # %reset_selective -f \"^timeseries_df_train$\"\n",
    "# # %reset_selective -f \"^timeseries_df_val$\"\n",
    "# # %reset_selective -f \"^timeseries_df_test$\"\n",
    "# # %reset_selective -f \"^df$\"\n",
    "# del timeseries_df_train\n",
    "# del timeseries_df_val\n",
    "# del timeseries_df_test\n",
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All-in-one-go Data Windowing\n",
    "# \n",
    "# df_train = TensorDataset(\n",
    "#     torch.Tensor(df_train[\"X\"]),\n",
    "#     torch.Tensor(df_train[\"Y\"])\n",
    "# )\n",
    "# data_loader_train = DataLoader(\n",
    "#     df_train,\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = True,\n",
    "#     num_workers = 10\n",
    "# )\n",
    "\n",
    "# df_val = TensorDataset(\n",
    "#     torch.Tensor(df_val[\"X\"]),\n",
    "#     torch.Tensor(df_val[\"Y\"])\n",
    "# )\n",
    "# data_loader_val = DataLoader(\n",
    "#     df_val,\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = True,\n",
    "#     num_workers = 10\n",
    "# )\n",
    "\n",
    "# df_test = TensorDataset(\n",
    "#     torch.Tensor(df_test[\"X\"]),\n",
    "#     torch.Tensor(df_test[\"Y\"])\n",
    "# )\n",
    "# data_loader_test = DataLoader(\n",
    "#     df_test,\n",
    "#     batch_size = batch_size,\n",
    "#     shuffle = True,\n",
    "#     num_workers = 10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### TRAIN #####\n",
    "\n",
    "num_train_samples = df_train.shape[0]\n",
    "\n",
    "input_df = df_train.select(\n",
    "    pl.col(input_features)\n",
    ")\n",
    "label_df = df_train.select(\n",
    "    pl.col(label_features)\n",
    ")\n",
    "input_df = input_df.explode(\"*\").to_numpy()\n",
    "input_df = input_df.reshape(num_train_samples, num_single_sample_timesteps, len(input_features))\n",
    "label_df = label_df.explode(\"*\").to_numpy()\n",
    "label_df = label_df.reshape(num_train_samples, num_single_sample_timesteps, len(label_features))\n",
    "\n",
    "df_train = WindowedDataset(\n",
    "    input_df = input_df,\n",
    "    label_df = label_df,\n",
    "    num_single_sample_timesteps = num_single_sample_timesteps,\n",
    "    stride = window_stride,\n",
    "    input_window_length = input_window_length,\n",
    "    label_window_length = label_window_length\n",
    ")\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    df_train,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 10,\n",
    "    prefetch_factor = 8,\n",
    "    persistent_workers = True,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "##### VALIDATION #####\n",
    "num_val_samples = df_val.shape[0]\n",
    "\n",
    "input_df = df_val.select(\n",
    "    pl.col(input_features)\n",
    ")\n",
    "label_df = df_val.select(\n",
    "    pl.col(label_features)\n",
    ")\n",
    "input_df = input_df.explode(\"*\").to_numpy()\n",
    "input_df = input_df.reshape(num_val_samples, num_single_sample_timesteps, len(input_features))\n",
    "label_df = label_df.explode(\"*\").to_numpy()\n",
    "label_df = label_df.reshape(num_val_samples, num_single_sample_timesteps, len(label_features))\n",
    "\n",
    "df_val = WindowedDataset(\n",
    "    input_df = input_df,\n",
    "    label_df = label_df,\n",
    "    num_single_sample_timesteps = num_single_sample_timesteps,\n",
    "    stride = window_stride,\n",
    "    input_window_length = input_window_length,\n",
    "    label_window_length = label_window_length\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    df_val,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 10,\n",
    "    prefetch_factor = 8,\n",
    "    persistent_workers = True,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "##### TEST #####\n",
    "num_test_samples = df_test.shape[0]\n",
    "\n",
    "input_df = df_test.select(\n",
    "    pl.col(input_features)\n",
    ")\n",
    "label_df = df_test.select(\n",
    "    pl.col(label_features)\n",
    ")\n",
    "input_df = input_df.explode(\"*\").to_numpy()\n",
    "input_df = input_df.reshape(num_test_samples, num_single_sample_timesteps, len(input_features))\n",
    "label_df = label_df.explode(\"*\").to_numpy()\n",
    "label_df = label_df.reshape(num_test_samples, num_single_sample_timesteps, len(label_features))\n",
    "\n",
    "df_test = WindowedDataset(\n",
    "    input_df = input_df,\n",
    "    label_df = label_df,\n",
    "    num_single_sample_timesteps = num_single_sample_timesteps,\n",
    "    stride = window_stride,\n",
    "    input_window_length = input_window_length,\n",
    "    label_window_length = label_window_length\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    df_test,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 10,\n",
    "    prefetch_factor = 8,\n",
    "    persistent_workers = True,\n",
    "    pin_memory = True\n",
    ")\n",
    "\n",
    "\n",
    "##### CLEANING UP! #####\n",
    "\n",
    "del input_df\n",
    "del label_df\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model: 135361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   2%|▏         | 32/1880 [00:28<27:25,  1.12it/s, val_loss=0.284753] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_label_timesteps):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m#tgt_mask = model.transformer.generate_square_subsequent_mask(decoder_input.shape[1]).to(device) --> Alternative to full mask! (Indexing is faster than mask generation!)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     tgt_mask \u001b[38;5;241m=\u001b[39m full_mask[:t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, :t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     next_step \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    156\u001b[0m     outputs[:, t, :] \u001b[38;5;241m=\u001b[39m next_step\n",
      "File \u001b[0;32m~/Documents/Thesis/src/thesis-3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:736\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.decode\u001b[0;34m(self, tgt, memory, tgt_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(tgt_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m     tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(tgt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 56\u001b[0m decoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask \u001b[38;5;241m=\u001b[39m tgt_mask)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_proj(decoder_out)\n",
      "File \u001b[0;32m~/Documents/Thesis/src/thesis-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Thesis/src/thesis-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Thesis/src/thesis-3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:625\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    622\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[1;32m    624\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m _get_seq_len(tgt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m--> 625\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m \u001b[43m_detect_is_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    628\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    629\u001b[0m         output,\n\u001b[1;32m    630\u001b[0m         memory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m         memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[1;32m    637\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Thesis/src/thesis-3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:1230\u001b[0m, in \u001b[0;36m_detect_is_causal_mask\u001b[0;34m(mask, is_causal, size)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# Do not use `torch.equal` so we handle batched masks by\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;66;03m# broadcasting the comparison.\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m causal_comparison\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 1230\u001b[0m     make_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_comparison\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     make_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TimeSeriesTransformer(\n",
    "    input_dim = len(input_features),\n",
    "    output_dim = len(label_features),\n",
    "    d_model = embedding_dim,\n",
    "    num_head = num_attention_head,\n",
    "    num_encoder_layers = num_encoder_layers,\n",
    "    num_decoder_layers = num_encoder_layers,\n",
    "    positional_encoding_max_len = positional_encoding_max_len,\n",
    "    position_wise_ffn_dim = position_wise_nn_dim,\n",
    "    dropout = dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"Number of trainable parameters in the model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\\n\")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate\n",
    ")\n",
    "\n",
    "train_r2 = R2Score(multioutput = \"uniform_average\").to(device)\n",
    "val_r2 = R2Score(multioutput = \"uniform_average\").to(device)\n",
    "\n",
    "overfit_count = 0\n",
    "\n",
    "model.forward = torch.compile(model.forward)    # Faster for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ################################################## TRAINING ##################################################\n",
    "    ##### No Scheduled Sampling #####\n",
    "    # model.train()\n",
    "    # epoch_train_loss = 0.0\n",
    "    # train_progress_bar = tqdm(\n",
    "    #     data_loader_train,\n",
    "    #     desc = f\"Epoch {epoch + 1}/{epochs}\"\n",
    "    # )\n",
    "    \n",
    "    # for batch_x, batch_y in train_progress_bar:\n",
    "    #     batch_x = batch_x.to(device)\n",
    "    #     batch_y = batch_y.to(device)\n",
    "\n",
    "    #     decoder_input = torch.zeros_like(batch_y)\n",
    "    #     decoder_input[:, 1:] = batch_y[:, :-1]\n",
    "    #     decoder_input[:, 0] = 0    # Adding bos token\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     output = model(batch_x, decoder_input)\n",
    "    #     loss = criterion(output, batch_y)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     epoch_train_loss += loss.item()\n",
    "    #     train_r2.update(\n",
    "    #         output.view(output.shape[0], -1),    # Flatten (batch_size, timestep * num_feature)\n",
    "    #         batch_y.view(batch_y.shape[0], -1)\n",
    "    #     )\n",
    "    #     train_progress_bar.set_postfix({\n",
    "    #         \"train_loss\": f\"{loss.item():.6f}\"\n",
    "    #     })\n",
    "\n",
    "    # avg_train_loss = epoch_train_loss / len(data_loader_train)\n",
    "    # epoch_train_r2 = train_r2.compute()\n",
    "    # train_r2.reset()\n",
    "    # print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Train R2: {epoch_train_r2:.6f}\")\n",
    "    #################################\n",
    "\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    train_progress_bar = tqdm(\n",
    "        data_loader_train,\n",
    "        desc = f\"Epoch {epoch + 1}/{epochs}\"\n",
    "    )\n",
    "\n",
    "    epsilon = max(0, 1.0 - 10 * epoch / epochs)  # Linear decay for choosing between label or pred as decoder input\n",
    "\n",
    "    for batch_x, batch_y in train_progress_bar:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        num_label_batch_samples, num_label_timesteps, num_label_features = batch_y.shape\n",
    "\n",
    "        # First pass: Just get preds --> No grads!\n",
    "        decoder_input_first_pass = torch.zeros_like(batch_y)\n",
    "        decoder_input_first_pass[:, 1:, :] = batch_y[:, :-1, :] # [:, 0, :] BOS = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_first_pass = model(batch_x, decoder_input_first_pass)\n",
    "            output_first_pass = output_first_pass.detach()   # detach: Not involved in model computation graph\n",
    "\n",
    "        # Second pass: pred and label decision for decoder input and back prop\n",
    "        decoder_input_second_pass = torch.zeros_like(batch_y)   # [;, 0, :] BOS = 0\n",
    "        teacher_force_mask = (\n",
    "            torch.rand(num_label_batch_samples, num_label_timesteps - 1, device = device) < epsilon    # Not including BOS\n",
    "        ).unsqueeze(2)  # Boolean mat of size: (num_label_batch_samples, num_label_timesteps, 1) --> Bool for selecting either pred or label\n",
    "        decoder_input_second_pass[:, 1:, :] = torch.where(\n",
    "            teacher_force_mask,\n",
    "            batch_y[:, :-1, :],\n",
    "            output_first_pass[:, :-1, :]\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        output_second_pass = model(batch_x, decoder_input_second_pass)\n",
    "        loss = criterion(output_second_pass, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_r2.update(\n",
    "            output_second_pass.view(output_second_pass.shape[0], -1),    # Flatten (batch_size, timestep * num_feature)\n",
    "            batch_y.view(batch_y.shape[0], -1)\n",
    "        )\n",
    "        train_progress_bar.set_postfix({\n",
    "            \"train_loss\": f\"{loss.item():.6f}\"\n",
    "        })\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(data_loader_train)\n",
    "    epoch_train_r2 = train_r2.compute()\n",
    "    train_r2.reset()\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Train R2: {epoch_train_r2:.6f}\")\n",
    "\n",
    "    ################################################# VALIDATION #################################################\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    val_progress_bar = tqdm(\n",
    "        data_loader_val,\n",
    "        desc = f\"Epoch {epoch + 1}/{epochs}\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_progress_bar:\n",
    "            # No teacher forcing in inference: Output at each timestep is feeded back to the decoder as input in the next timestep\n",
    "            # Attention mask grows as timesteps pass\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            num_label_batch_samples, num_label_timesteps, num_label_features = batch_y.shape    # num_label_features == len(label_features)\n",
    "\n",
    "            outputs = torch.zeros(num_label_batch_samples, num_label_timesteps, num_label_features).to(device)\n",
    "            decoder_input = torch.zeros(num_label_batch_samples, 1 + num_label_timesteps, num_label_features).to(device)    # +1 for BOS\n",
    "\n",
    "            full_mask = model.transformer.generate_square_subsequent_mask(1 + num_label_timesteps).to(device)\n",
    "            encoder_output = model.encode(batch_x)\n",
    "\n",
    "            for t in range(num_label_timesteps):\n",
    "                # tgt_mask = model.transformer.generate_square_subsequent_mask(decoder_input.shape[1]).to(device) --> Alternative to full mask! (Indexing is faster than mask generation!)\n",
    "                tgt_mask = full_mask[:t+1, :t+1]\n",
    "                out = model.decode(\n",
    "                    tgt = decoder_input[:, :t+1, :],\n",
    "                    memory = encoder_output,\n",
    "                    tgt_mask = tgt_mask\n",
    "                )\n",
    "                next_step = out[:, -1, :]\n",
    "                outputs[:, t, :] = next_step\n",
    "                decoder_input[:, t+1, :] = next_step\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            epoch_val_loss += loss.item()\n",
    "            val_r2.update(\n",
    "                outputs.view(outputs.shape[0], -1),    # Flatten (batch_size, timestep * num_feature)\n",
    "                batch_y.view(batch_y.shape[0], -1)\n",
    "            )\n",
    "            val_progress_bar.set_postfix({\n",
    "                \"val_loss\": f\"{loss.item():.6f}\"\n",
    "            })\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(data_loader_val)\n",
    "        epoch_val_r2 = val_r2.compute()\n",
    "        val_r2.reset()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Val Loss: {avg_val_loss:.6f}, Val R2: {epoch_val_r2:.6f}\\n\")\n",
    "    \n",
    "\n",
    "    if(epoch >= 10 and avg_val_loss - avg_train_loss > 0.01):\n",
    "        overfit_count += 1\n",
    "        print(f\"Possible Overfitting!!! {overfit_count}/3\\n\")\n",
    "        if(overfit_count == 3):\n",
    "            print(\"Training Stopped!!!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 940/940 [12:44<00:00,  1.23it/s, batch_test_loss=0.042307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 0.098021, Final Test R2: -2.319430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################## TESTING ##################################################\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_progress_bar = tqdm(\n",
    "    data_loader_test\n",
    ")\n",
    "\n",
    "test_r2 = R2Score(multioutput = \"uniform_average\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_progress_bar:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        num_label_batch_samples, num_label_timesteps, num_label_features = batch_y.shape    # num_label_features == len(label_features)\n",
    "\n",
    "        outputs = torch.zeros(num_label_batch_samples, num_label_timesteps, num_label_features).to(device)\n",
    "        decoder_input = torch.zeros(num_label_batch_samples, 1 + num_label_timesteps, num_label_features).to(device)    # +1 for BOS\n",
    "\n",
    "        full_mask = model.transformer.generate_square_subsequent_mask(1 + num_label_timesteps).to(device)\n",
    "        encoder_output = model.encode(batch_x)\n",
    "\n",
    "        for t in range(num_label_timesteps):\n",
    "            tgt_mask = full_mask[:t+1, :t+1]\n",
    "            out = model.decode(\n",
    "                tgt = decoder_input[:, :t+1, :],\n",
    "                memory = encoder_output,\n",
    "                tgt_mask = tgt_mask\n",
    "            )\n",
    "            next_step = out[:, -1, :]\n",
    "            outputs[:, t, :] = next_step\n",
    "            decoder_input[:, t+1, :] = next_step\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        test_progress_bar.set_postfix({\n",
    "            \"batch_test_loss\": f\"{loss.item():.6f}\"\n",
    "        })\n",
    "        test_r2.update(\n",
    "            outputs.view(outputs.shape[0], -1),\n",
    "            batch_y.view(batch_y.shape[0], -1)\n",
    "        )\n",
    "\n",
    "    final_test_loss = test_loss / len(data_loader_test)\n",
    "    final_test_r2 = test_r2.compute()\n",
    "    test_r2.reset()\n",
    "    print(f\"Final Test Loss: {final_test_loss:.6f}, Final Test R2: {final_test_r2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
